---
---
References
==========

@misc{albergo_stochastic_2023,
	title = {Stochastic {Interpolants}: {A} {Unifying} {Framework} for {Flows} and {Diffusions}},
	shorttitle = {Stochastic {Interpolants}},
	url = {http://arxiv.org/abs/2303.08797},
	abstract = {A class of generative models that uniﬁes ﬂow-based and diﬀusion-based methods is introduced. These models extend the framework proposed in [1], enabling the use of a broad class of continuoustime stochastic processes called ‘stochastic interpolants’ to bridge any two arbitrary probability density functions exactly in ﬁnite time. These interpolants are built by combining data from the two prescribed densities with an additional latent variable that shapes the bridge in a ﬂexible way. The time-dependent probability density function of the stochastic interpolant is shown to satisfy a ﬁrst-order transport equation as well as a family of forward and backward Fokker-Planck equations with tunable diﬀusion. Upon consideration of the time evolution of an individual sample, this viewpoint immediately leads to both deterministic and stochastic generative models based on probability ﬂow equations or stochastic diﬀerential equations with an adjustable level of noise. The drift coeﬃcients entering these models are time-dependent velocity ﬁelds characterized as the unique minimizers of simple quadratic objective functions, one of which is a new objective for the score of the interpolant density. Remarkably, we show that minimization of these quadratic objectives leads to control of the likelihood for any of our generative models built upon stochastic dynamics. By contrast, we establish that generative models based upon a deterministic dynamics must, in addition, control the Fisher divergence between the target and the model. We also construct estimators for the likelihood and the cross-entropy of interpolant-based generative models, discuss connections with other stochastic bridges, and demonstrate that such models recover the Schr¨odinger bridge between the two target densities when explicitly optimizing over the interpolant.},
	language = {en},
	urldate = {2023-07-25},
	publisher = {arXiv},
	author = {Albergo, Michael S. and Boffi, Nicholas M. and Vanden-Eijnden, Eric},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08797 [cond-mat]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Mathematics - Probability},
	file = {Albergo et al. - 2023 - Stochastic Interpolants A Unifying Framework for .pdf:/local/scratch/telfaralex/Zotero/storage/ZIMWLRQ9/Albergo et al. - 2023 - Stochastic Interpolants A Unifying Framework for .pdf:application/pdf},
}

@misc{liu_flow_2022,
	title = {Flow {Straight} and {Fast}: {Learning} to {Generate} and {Transfer} {Data} with {Rectified} {Flow}},
	shorttitle = {Flow {Straight} and {Fast}},
	url = {http://arxiv.org/abs/2209.03003},
	abstract = {We present rectiﬁed ﬂow, a surprisingly simple approach to learning (neural) ordinary differential equation (ODE) models to transport between two empirically observed distributions π0 and π1, hence providing a uniﬁed solution to generative modeling and domain transfer, among various other tasks involving distribution transport. The idea of rectiﬁed ﬂow is to learn the ODE to follow the straight paths connecting the points drawn from π0 and π1 as much as possible. This is achieved by solving a straightforward nonlinear least squares optimization problem, which can be easily scaled to large models without introducing extra parameters beyond standard supervised learning. The straight paths are special and preferred because they are the shortest paths between two points, and can be simulated exactly without time discretization and hence yield computationally efﬁcient models. We show that the procedure of learning a rectiﬁed ﬂow from data, called rectiﬁcation, turns an arbitrary coupling of π0 and π1 to a new deterministic coupling with provably non-increasing convex transport costs. In addition, recursively applying rectiﬁcation allows us to obtain a sequence of ﬂows with increasingly straight paths, which can be simulated accurately with coarse time discretization in the inference phase. In empirical studies, we show that rectiﬁed ﬂow performs superbly on image generation, image-to-image translation, and domain adaptation. In particular, on image generation and translation, our method yields nearly straight ﬂows that give high quality results even with a single Euler discretization step.},
	language = {en},
	urldate = {2023-08-03},
	publisher = {arXiv},
	author = {Liu, Xingchao and Gong, Chengyue and Liu, Qiang},
	month = sep,
	year = {2022},
	note = {arXiv:2209.03003 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Liu et al. - 2022 - Flow Straight and Fast Learning to Generate and T.pdf:/local/scratch/telfaralex/Zotero/storage/F7SEBUCV/Liu et al. - 2022 - Flow Straight and Fast Learning to Generate and T.pdf:application/pdf},
}


@article{lipman_flow_2023,
	title = {{FLOW} {MATCHING} {FOR} {GENERATIVE} {MODELING}},
	abstract = {We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples—which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.},
	language = {en},
	author = {Lipman, Yaron and Chen, Ricky T Q and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt},
	year = {2023},
	file = {Lipman et al. - 2023 - FLOW MATCHING FOR GENERATIVE MODELING.pdf:/local/scratch/telfaralex/Zotero/storage/4EP7JY9I/Lipman et al. - 2023 - FLOW MATCHING FOR GENERATIVE MODELING.pdf:application/pdf},
}

@misc{dao2023flowmatchinglatentspace,
      title={Flow Matching in Latent Space}, 
      author={Quan Dao and Hao Phung and Binh Nguyen and Anh Tran},
      year={2023},
      eprint={2307.08698},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2307.08698}, 
}

@misc{esser2024scalingrectifiedflowtransformers,
      title={Scaling Rectified Flow Transformers for High-Resolution Image Synthesis}, 
      author={Patrick Esser and Sumith Kulal and Andreas Blattmann and Rahim Entezari and Jonas Müller and Harry Saini and Yam Levi and Dominik Lorenz and Axel Sauer and Frederic Boesel and Dustin Podell and Tim Dockhorn and Zion English and Kyle Lacey and Alex Goodwin and Yannik Marek and Robin Rombach},
      year={2024},
      eprint={2403.03206},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2403.03206}, 
}