---
title: The behaviour of neural flows
layout: post
subtitle: Neural nets can struggle to learn very simple flows.
permalink: /si-in-practice/
scholar:
  bibliography: "ideal-si.bib"
categories: 
  - "research"
---

<!-- 
issues i want to cover;

- width issues and dimensionality.
- how does stochasticity / noise help / hurt? (variance of the grad estimates. smoothness of the flow)
- how close is the learned vfield to the exact solution? is there a global minima?
- 
 -->

Stocahastic interpolants are a recent innovation that frames generative modelling as building a transport map between distributions. {% cite albergo_stochastic_2023 liu_flow_2022 lipman_flow_2023 %}

We are given two distributions, $p(x)$ and $q(x)$ over the same space $X$. Our goal is to find a vector field $v$ that allows us to map from $p(x)$ to $q(x)$.

***

I have found that in practice; the behaviour of approximations to flows (using neural networks) was unexpected.

> neural networks can struggle to approximate very simple flows.

<!-- 
the alignment of the vector fields and the KL div can be highly uncorrelated?! 
training doesnt really care about low probability regions. p_t(z)
-->


## Struggling to learn simple mappings

<side>the -only?- solution being some kind of space-filling curve or pseudo random mapping?.</side>
We provide two illustrative examples, which are both exploit the same core issue. It's hard to learn to (deterministically) generate variation in $k$ dimensions from a variable in $n$ dimensions, where $k > n$.

### Not enough width (in the neural network)

Here we show that mapping between two zero-mean isotropic Gaussians in $k$ dimensions is hard if $k > w$. Where $w$ is the smallest width of the neural network.

With (lots) more training, the neural network can get better, but it's still not perfect.
A high dimensional Gaussian is surprisingly hard to learn.

![]({{ site.baseurl }}/assets/neural-si/gaussian-widths-epochs.png)

### Not enough variance (in the data)

Here we learn a flow from;

1. moons to 8-Gaussian dataset
2. line to 8-Gaussian dataset

while keeping every other hyperparameter the same (neural network architecture, optimizer, learning rate, etc).

![si]({{ site.baseurl }}/assets/neural-si/8g-moons.png)

> Top left: the data distribution, bottom right: the target distribution. Top right: the data distribution mapped to the target distribution using a learned flow. Bottom left: the target distribution mapped to the data distribution using the inverse of the learned flow.

![si]({{ site.baseurl }}/assets/neural-si/8g-line.png)

> Similar organisation to the previous figure. However we see that the learned flow is not able to map the line to the 8-Gaussian dataset.

Again, but in higher dimensions we demonstrate this issue. Here we define our data distribution to be a istropoic, zero mean, Gaussian in 64 or 128 dimensions with 0.5 variance. We define our target distribution to be a isotropic, zero mean, Gaussian in 128 dimensions with 1.0 variance.

![pic]({{ site.baseurl }}/assets/neural-si/64-128-ranks.png)

> Here we have plotted the histogram of norms of the forward flow, $\{\parallel f(x) \parallel: x \in p(x)\}$. We have also plotted the Chi distribution, which is the analytical solution to this problem.

***

In practice this is a problem because many datasets (eg MNIST) do not have 784 dimensions of variation (by computing the rank of the covariance of the MNIST train set we find that it has 228 independent dimensions of variation). Another way of stating this is that the data lie on a low-dimensional manifold.

***

If we allow ourselves to use randomness, then this problem quickly disappears. Thus why diffusion models do not suffer from this issue.

An alternative solution is to use latent flows.
By projecting the data into a lower dimension where all the variation is captured we avoid this issue.

<!-- (could also swap the target distribution for a lower dimensional one) -->

Latent flows appear to be in common use {% cite dao2023flowmatchinglatentspace esser2024scalingrectifiedflowtransformers %}, however the reasons for using them is not often understood (other than increases in performance).

## Bibliography

{% bibliography --cited %}

