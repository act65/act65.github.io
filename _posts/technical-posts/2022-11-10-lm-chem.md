---
layout: post
title: Language models are all you need
subtitle: for SMILES-based chemistry
permalink: lm-chem
categories:
  - proposal
scholar:
  bibliography: "lm-chem.bib"
---

> Let's train a single large language model on for as many different chemistry tasks as possible.

<side>
This post was adapted from a presentation.
</side>

Let's imagine a few tasks;

### Property prediction

For examples, tasks could be;
- "The logP of C1=CC=NC=C1 is " __->__ "0.65"
- "The boiling point of CC(C)Cc1ccc(C(C)C(=O)O)cc1 at 1atm is" __->__ "157 C"
- any / all physical properties. (molecular weight, melting point, pKa, NMR shifts, refractive index, ... )

There are many sources of data for these tasks, for example; [Reaxys](https://www.reaxys.com/#/search/quick), [Pubchem](https://pubchem.ncbi.nlm.nih.gov/).


### (graph) structure elucidation

We could prompt the LLM with;
- Prompt: _"Molecular formula: C4H6O4. Observed 13C NMR peaks: 28.9, 173.9. Elucidated structure: \_\_\_\_\_\_\_\_\_\_"_<br>
- Completion: _"C(CC(=O)O)C(=O)O"_

Or;
- _"Molecular formula: C21H22N2O2. Observed 13C NMR peaks: 1.22, 1.41, 1.83, 1.84, ... Elucidated structure: \_\_\_\_\_\_\_\_\_\_"_<br>
- Completion: _"O=C1CC2OCC=C3C4C2C2N1c1ccccc1C12CCN(C1C4)C3"_

We could use data from NMRShiftDB {% cite Kuhn2015FacilitatingQC %} for this task.

### Translation from natural language to synthetic 'program'

The input prompt could be;

- Prompt: _"Text: A vial was charged with LiCl (0.32 g) in anhydrous THF (15 mL). Program: \_\_\_\_\_\_\_\_\_\_"_ <br>
- Completion: _"\<AddSolid vessel="reactor" reagent="LiCl" mass="0.32 g" \/\>\<Add vessel="reactor" reagent="THF" volume="15 mL" \/\>"_

A dataset for this could be sourced from [orgsyn](http://www.orgsyn.org/) or [Pistachio_2017](https://www.nextmovesoftware.com/pistachio.html).

***

What are other tasks we could include?

- reactivity?
- drug design?
- retrosynthesis?

> All of chemistry in a LLM!

Future work could investigate how to include other types of chemical data. For example;

- [QM9](http://quantum-machine.org/datasets/) (3D positions, electron densities)
- [Open catalyst project](https://opencatalystproject.org/) (trajectories, forces and energies)
- Theoretical knowledge. Like the dataset used [here](https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/6393827c836cebbc757aedeb/original/assessment-of-chemistry-knowledge-in-large-language-models-that-generate-code.pdf) or what about past exams?


## Motivation / background

Recently there have been two important observations with AI;

1. Deep learning works when done at scale. The bigger the better.
2. The ability of strings to represent many different kinds of information allows great flexibility which tasks a LLM can be trained to do.

### Deep learning at scale

- More parameters
  - Gopher {% cite Rae2021ScalingLM %}. 280B parameters
  - PaLM {% cite Chowdhery2022PaLMSL %}. 540B parameters
  - Switch Transformers {% cite Fedus2021SwitchTS %}. 1T parameters
- More data
  - MassiveText {% cite Rae2021ScalingLM %}. 2 trillion tokens
  - LTIP {% cite schuhmann2021laion400mopendatasetclipfiltered %}. 400 million image-caption pairs
- More tasks
  - A generalist agent GATO {% cite reed2022generalistagent %}. Trained on;
    - Simulated control tasks (596 tasks) ([DM Control](https://github.com/deepmind/dm_control), [DM lab](https://www.deepmind.com/open-source/deepmind-lab), [Procgen](https://openai.com/blog/procgen-benchmark/), [Atari ALE](https://github.com/mgbellemare/Arcade-Learning-Environment),  [playroom](https://arxiv.org/abs/1707.03300), ... and more)
    - vision and language (>204 tasks) ([MassiveText](https://arxiv.org/abs/2112.11446), [MultiModal MassiveWeb](https://arxiv.org/abs/2204.14198), [LTIP](https://arxiv.org/abs/2111.02114), [OKVQA](https://okvqa.allenai.org/), ... and more)


### The flexibility of strings and power of LLMs

A language model is a 'model' of language. It models language as a prediction problem: given context predict a distribution over the likely next token.

- "The cat in the _" __->__ "hat" (probably)
- "Monday, Tuesday, Wednesday, _" __->__ Thursday (probably)
- "The most populous city in India is _" __->__ Mumbai (probably)

Given a language model, we can frame many NLP tasks as predicting the next token, given a prompt.
This lead some to claim _"Language models are all you need"_ (for NLP tasks) {% cite namazifar2020languagemodelneednatural %}.

For example; narrative understanding, textual entailment, entity resolution, question answering, POS tagging, grammatic parsing... can all be framed as predicting the next token. And thus can be done with a LLM.

- textual entailment: "text: If you help the needy, God will reward you. hypothesis: Giving money to a poor man has good consequences." -> "positive"  (text entails hypothesis)
- POS tagging: "text: Bob made a book collector happy." -> "subject verb object(article adjective noun) verb-modifier"
- sentiment analysis: "text: I love this movie!" -> "positive"
- question answering: "text: The capital of France is _" -> "Paris"

## Aside: more fun tasks LMs can do

_Big-Bench_ {% cite srivastava2023imitationgamequantifyingextrapolating %}

Analyses a LM's ability to do 204 different tasks including.

- auto_debugging
  - "'\\nfor i in range(10):\\n\\ti' What is the value of i the third time line 2 is executed?" __->__ "2"
- color matching
  - "What is the color most closely matching this RGB representation: rgb(128, 2, 198)?" __->__ "purple"
- chess_state_tracking_legal_moves
  - "e2e4 g7g6 d2d4 f8g7 c1e3 g8f6 f2f3 d7d6 d1" __->__ "c1, d2, d3, e2"

others
- ascii mnist, solve riddles, play sudoku, translate hindi proverbs, idendify math thorems, vitamin C fact verification, etc...

<!-- ## Aside: Few-shot meta learning vs fine-tuning

Fine-tune

_What is human life expectancy in the United States?" -> "Human life expectancy in the United States is 78 years._

(we actually train the LM on examples: question -> answer)

Zero-shot

_I am a highly intelligent question answering bot. Ask me questions, I will answer._

_Q: What is human life expectancy in the United States?_

## Aside: Few-shot meta learning vs fine-tuning

Few shot

_I am a highly intelligent question answering bot. Ask me questions, I will answer._

_Q: Who was president of the United States in 1955?_\
_A: Dwight D. Eisenhower was president of the United States in 1955._

_Q: Where were the 1992 Olympics held?_\
_A: The 1992 Olympics were held in Barcelona, Spain._

_Q: What is human life expectancy in the United States?_

(example from openai's api) -->

## Conclusion

Molecules and analytical data can be represented as strings. This allows us to use LLMs to do chemistry tasks.
But how can we cram enough knowledge into a LLM to make it useful for chemistry?
Let's train a single large language model on for as many different chemistry tasks as possible.

## Bibliography

{% bibliography --cited %}