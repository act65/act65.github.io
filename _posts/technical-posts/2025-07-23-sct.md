---
title: A Decision-Theoretic Take on Social Choice
subtitle: Using L2 to solve the preference dilution problem
layout: post
categories:
    - research
---

Voting seems simple. You count the votes, and the option with the most votes wins. Yet, as anyone who has followed an election knows, the reality is a messy, often paradoxical affair. The field that formally studies this mess is Social Choice Theory, and it's full of surprising and unsettling results that tell us there is no "perfect" voting system.

But what if we've been thinking about the problem from the wrong angle?

Traditional Social Choice Theory often focuses on the paradoxes that arise from ranking candidates. But in an age of data and computation, we can reframe the problem: what if making a collective choice is not about ranking, but about **approximating the best possible outcome from noisy, incomplete data?**

This article will walk through that decision-theoretic framework. We'll explore the core ways to define the "best" outcome, diagnose a fundamental flaw in our standard methods, and propose a solution based on a more expressive way to measure preference.

### The Goal: Social Choice as an Optimization Problem

Let's start with a powerful assumption. Imagine we have an oracle that can tell us the true, numerical "utility" or "preference" every single person has for every possible option. Let $u_i(x)$ be the true utility person $i$ gets from outcome $x$.

If we had this god-like view, our task would become a straightforward optimization problem. But what exactly are we optimizing? There are two main schools of thought:

1.  **Utilitarian (or Benthamite) Welfare:** We aim to maximize the *total* happiness in society. The best outcome is the one that wins the following contest:

    $$x^* = \underset{x}{\text{argmax}} \sum_i u_i(x), u \in \mathbb R$$

    This is a simple sum. A huge gain for one person can easily outweigh small losses for many others.

2.  **Nash Welfare:** We aim to maximize the *product* of everyone's happiness. This has a profoundly different ethical implication:

    $$x^* = \underset{x}{\text{argmax}} \prod_i u_i(x), \u \in [0,1]$$

    Because it's a product, an outcome that gives one person a utility of zero (or close to it) gets a massive penalty, as the entire product goes to zero. This system is inherently inequality-averse; it searches for solutions that everyone can, at the very least, live with.[^1]

It's worth noting how both these frameworks, despite their differences, are vulnerable to a kind of "preference cancellation." In a utilitarian sum, a person with a strong positive preference ($+100$) can be completely cancelled out by two people with moderate negative preferences ($-50$ each). In a Nash product, which requires positive utility values to work, a single person with a utility of zero for an outcome can veto it for everyone, regardless of how much others desire it. This sensitivity makes the *estimation* of true preferences a critical challenge.

_**A Note on Traditional Social Choice Theory:** This framing is a departure from the classical view. Pioneers like Kenneth Arrow didn't assume we could know or compare numerical utilities between people. His famous Impossibility Theorem proved that even if we only ask for simple rankings (ordinal preferences), no voting system can satisfy a handful of basic fairness criteria simultaneously.[^2] Our approach here is different: we assume a "ground truth" cardinal utility exists, and the challenge is one of *estimation*. This itself is a departure from stricter economic views that are skeptical of interpersonal utility comparisons._

In this framework, we have separated our concerns:
We have an optimisation problem (maximising social utility) and an estimation problem (preference elicitation).

### The Problem: Loudmouths, Dilution, and Normalization

Of course, we don't have an oracle. We have to ask people for their preferences, and this is where the trouble begins!

*   **Unnormalized Preferences:** If we just let people report a number, everyone has an incentive to be a "loudmouth"—to report exaggeratedly high or low numbers to pull the final outcome in their direction. It also feels fundamentally unfair: why should one person's preference scale be vastly larger than another's?

*   **Normalized Preferences (The Obvious Fix):** The intuitive solution is to give everyone an equal "budget" of preference. We can force each person's preferences to sum to one: $\sum_x p_i(x) = 1$. This is known as L1 normalization. It seems fair and elegant.

But this solution creates a subtle and devastating flaw: **the dilution problem.**

Consider an election with three candidates:
*   **Candidate A & B:** Two similar, center-ground candidates.
*   **Candidate C:** A very different, polarizing candidate.

Imagine the electorate is split:
*   **60% of voters** are "Generalists." They dislike C but find A and B both perfectly acceptable.
*   **40% of voters** are "Specialists." They are passionate, single-issue supporters of C.

How do the Generalists use their normalized preference budget of 1.0? They must split it: $p(A)=0.5, p(B)=0.5, p(C)=0$. The Specialists, however, can pour their entire budget into C: $p(A)=0, p(B)=0, p(C)=1.0$.

If we now try to find the utilitarian optimum by summing these declared preferences, C wins in a landslide. The 60% majority who preferred either A or B to C have their collective will thwarted because the normalization rule forced them to dilute their support.

This outcome is even more troubling when we consider the concept of a **Condorcet winner**: a candidate who would defeat every other candidate in a direct head-to-head comparison.[^3] In our example, Candidate A would beat C by a 60-40 margin, as would Candidate B. Both A and B are Condorcet winners, yet the system picks C, the Condorcet *loser*. This is a clear failure of the aggregation method.

### A Better Normalization: Measuring Intensity with the L2 Norm

The L1 norm ($\sum p = 1$) failed because it links preferences together: more for A must mean less for B. Crucially, it provides no way to express dissatisfaction. The goal is to find a system that allows for this, effectively importing the "cancellation" power seen in Nash Welfare into the more flexible Utilitarian sum.

Let's change the rule. Instead of the sum of preferences being 1, let's demand that the **sum of the squares of the preferences** be 1.
$$\sum_x p_i(x)^2 = 1$$
This is called **L2 normalization**. This mathematical method has a famous parallel in quantum mechanics, where it is known as the Born rule.[^4] For our purposes, it has a powerful property:

**It allows for negative preferences.** Because the square of a negative number is positive, a voter can express opposition to a candidate (e.g., a preference of -0.5). The *intensity* of that opposition contributes to their normalization budget ($(-0.5)^2 = 0.25$), while the negative value itself is used in the final tally.

### Solving Dilution with L2 Normalization

Let's revisit our A, B, C election with this new rule. Each voter gets one "unit of intensity" to spend.

*   **The 40% Specialists (pro-C):** They feel passionately for C and neutrally about A and B. Their preference vector is $(0, 0, 1)$. (Check: $0^2 + 0^2 + 1^2 = 1$).

*   **The 60% Generalists (anti-C, pro-A/B):** They strongly support A and B and strongly oppose C. They can split their "intensity" across these feelings. A plausible vector would be $(0.7, 0.7, -0.2)$. (Check: $0.7^2 + 0.7^2 + (-0.2)^2 = 0.49 + 0.49 + 0.04 = 1.02 \approx 1$). They express strong, independent support for A and B, along with a clear opposition to C.

Now, let's find the collective will by summing the preference vectors of all 100 voters:

`Total Score = (40 * Specialist Vector) + (60 * Generalist Vector)`

*   **Score for A:** $(40 \times 0) + (60 \times 0.7) = 42$
*   **Score for B:** $(40 \times 0) + (60 \times 0.7) = 42$
*   **Score for C:** $(40 \times 1) + (60 \times -0.2) = 40 - 12 = 28$

The result is decisive. **Candidates A and B are the clear winners.** The system correctly interpreted the collective will. The ability for the majority to express independent support for multiple good options, while simultaneously expressing opposition to the bad one, completely solved the dilution problem.[^5]

However, this power is not without risk. A system that allows for negative preferences could also formalize a "tyranny of the majority." A cohesive majority could use the L2 framework to assign large, negative scores to a minority's preferred candidate, effectively using the sum to punish that choice. The objective function would not just select a winner, but could declare the minority's choice as having a negative social utility. This reveals a deep trade-off: protecting the majority from dilution versus protecting the minority from majoritarianism.

### The Future of Choice

By reframing social choice as an estimation problem, we move from a search for perfect rules to a search for the best approximation of a collective good. The common L1 normalization, while seemingly fair, contains a devastating flaw—preference dilution—that can cause the will of a majority to be defeated by a polarizing option.

Adopting an L2 normalization, where the sum of squared preferences is constant, provides a powerful solution. It allows voters to express independent support for multiple good options while also expressing opposition to poor ones, correctly identifying the Condorcet winner in our example. This framework effectively imports the "veto power" of a Nash Welfare system into the more tractable Utilitarian sum.

However, this solution is not without profound ethical trade-offs. A system that allows for negative preferences is also one that could formalize a "tyranny of the majority," allowing a large group to collectively punish a minority's choice. This highlights a fundamental tension in social choice: protecting against dilution versus protecting against majoritarianism.

While this post has focused on defining the *objective*, the path to implementation raises its own set of challenges, such as preference elicitation and strategic behavior. Yet, by first defining a more robust objective, we now have a clear goal against which we can measure any proposed mechanism. The L2 framework doesn't promise a perfect system, but it gives us a clearer picture of what we are trying to optimize, and what trade-offs we must confront to get there.

***

### References

[^1]: The Nash welfare function is derived from John Nash's work on bargaining, specifically the Nash Bargaining Solution, which seeks outcomes that are both efficient and equitable. See: Nash, J. F. (1950). "The Bargaining Problem." *Econometrica*, 18(2), 155–162.

[^2]: Arrow, K. J. (1951). *Social Choice and Individual Values*. Wiley. For a more accessible overview, the Stanford Encyclopedia of Philosophy entry on "Arrow's Theorem" is an excellent resource.

[^3]: The Marquis de Condorcet first described this paradox in the 18th century. A Condorcet winner is considered a very strong indicator of the collective will, and voting systems that can fail to elect an existing Condorcet winner are often seen as flawed.

[^4]: The Born rule, formulated by Max Born in 1926, is a fundamental principle in quantum mechanics stating that the probability of finding a quantum system in a given state is the square of the amplitude of the system's wavefunction. Our use here is a mathematical analogy, not a physical one.

[^5]: This L2-based objective is conceptually related to a practical voting system called **Quadratic Voting (QV)**, which also uses a quadratic relationship to measure preference intensity. However, a key difference is that QV typically does not allow for negative preferences and instead focuses on how individuals allocate a budget of 'voice credits,' solving a different but related problem of preference intensity and resource allocation. See: Posner, E. A., & Weyl, E. G. (2018). *Radical Markets: Uprooting Capitalism and Democracy for a Just Society*. Princeton University Press.