---
layout: post
title: Score functions, denoising and diffusion
subtitle: Diffusion is just stacked denoising score matching!
permalink: /scoredd/
scholar:
  bibliography: "scoredd.bib"
---

<!-- Goal: learn p(x) -->

Our goal is to approximate a data distribution $p(x)$ with a model $p_{\theta}(x)$ using a set of samples, $D_n = \{ x_i: x_i\sim p(x), i \in Z_n \}$.

<!-- Why do we care about p(x)? -->

With this model of the data distribution we can generate our own samples $x\sim p_{\theta}(x)$, and we can evaluate the likelihood $p_{\theta}(x)$. This ability to sample and evaluate facilitate many tasks in machine learning; generative modelling, density estimation, and anomaly detection.

***

<!-- Max likelihood -->

We start formulating the problem as finding a model distribution that minimises the KL divergence to the data distribution.

$$
D_{KL}(p, p_{\theta}) = \mathbb E_{x \sim p(x)} [ \log p(x) - \log p_{\theta}(x) ] \\
$$

The data distribution term $\log p(x)$ doesnt depend on $\theta$ and can be ignored when optimising $\theta$.

$$
\theta^{* } = \text{argmax}_{\theta} \mathbb E_{x \sim p(x)} [ \log p_{\theta}(x) ] \\
$$

Thus we are maximising the likelihood $p(x \mid \theta)$ of the data under the model.
Solving this optimisation problem is hard.
There are many different ways we can construct our model, $p_{\theta}(x)$. However, they require that:

$$
\int p_{\theta}(x) dx = 1
$$

This constraint is, in general, intractable. However, if we are clever, we can avoid this constraint. For example;

- _normalising flows_ construct invertible maps (that preserve density) between distributions. Thus, as long as the initial distribution is normalised, the final distribution will also be normalised.
- _variational autoencoders_ attempt to find a latent space where the data distribution is a simple (ie gaussian) distribution.
- _energy based models_ learn an energy function which corresponds to unnormalised probabilities.

<!-- Other approaches -->
In this blog, we consider yet another approach.

## Model the score indead of the density
<!-- BUT why learn the score? does it allow us to do the same things as p(x)? Yes. Allows sampling and evaluating p(x). -->
An alternative approach to modelling the density is to approximate the score function, $s(x, \theta) \approx \nabla_x \log p(x)$. This allows us to avoid the normalisation constraint.
Note, we can generate samples using a score function by using Langevin dynamics {% cite welling-teh-2011 %}.

Instead of the KL divergence (above) which led us to the maximum likelihood solution, we start with the Fisher divergence {% cite lyu_interpretation_2009 %}.

$$
\begin{align*}
D_F(p, p_\psi) &= \mathbb E_{x \sim p(x)} \parallel \frac{\nabla_x p(x)}{p(x)} - \frac{\nabla p_{\psi}(x)}{p_{\psi} (x)}\parallel^2 \\
\end{align*}
$$

This leads to the score matching objective function {% cite hyvarinen_estimation_2011 %} by using the log derivative trick.


$$
\begin{align*}
\mathcal L(\theta) &= \mathop{\mathbb E}_{x\sim p(x)} \parallel s(x, \theta) - \nabla_{x} \log p(x) \parallel^2 \\
\end{align*}
$$

However, this function contains $\nabla_x \log p(x)$, the true score function, which we do not have access to. 
Our setting only grants samples from the density. 

Fortunately, there are two known solutions to this problem; score matching {% cite hyvarinen_estimation_2011 %} and denoising score matching {% cite vincent_connection_2011 %}.



<aside>

What is the score function?

<!-- background on score functions -->
In the case where the density is a gaussian distribution, the score function is:

$$
\begin{align*}
p(x) &= \mathcal N(x \mid \mu, \sigma^2I) \\
\nabla_{x} \log p(x) &= \nabla_{x} \left( \log \frac{1}{\sqrt{2\pi \sigma^2}} \exp \left( - \frac{1}{2\sigma^2} \parallel x - \mu \parallel^2 \right) \right)\\
&= \nabla_{x} \left( - \frac{1}{2\sigma^2} \parallel x - \mu \parallel^2 \right) \\
&= \frac{1}{\sigma^2} (\mu - x) \\
\end{align*}
$$

<img src="{{ site.baseurl }}/assets/scoredd/gaussian-score.png" class="center-image" alt="The score function of a gaussian distribution.">

</aside>

### Score matching

{% cite hyvarinen_estimation_2011 %}
show that we can write the gradient of the score matching objective in a way that doesn't contain the score function. Thus allowing us to estimate the score function without having access to it.

The derivation is as follows;

$$
\begin{align*}
\nabla_\theta \mathcal L(\theta) &= \nabla_\theta \mathop{\mathbb E}_{x\sim p(x)} \Big[ \parallel s(x, \theta) \parallel^2 - 2 \langle s(x, \theta), \nabla_x \log p(x)\rangle + \parallel \nabla_x \log p(x)\parallel^2\Big]\\
&= \nabla_\theta \mathop{\mathbb E}_{x\sim p(x)} \Big[ \parallel s(x, \theta) \parallel^2 - 2 \langle s(x, \theta), \nabla_x \log p(x)\rangle\Big] \\
\end{align*}
$$

We epanded the score matching objective (from above). Now, the last term will drop off since it is not a function of $\theta$, thus we ignore it. Now we can use the log derivative trick (1.) and a partial integration trick (2.) to rewrite the second term.

$$
\begin{align*}
\int p(x)\langle s(x, \theta), \nabla_x \log p(x, \theta)\rangle dx &= \int p(x)\sum_i s(x, \theta)_i \nabla_{x_i} \log p(x) dx\\
&= \int p(x)\sum_i s(x, \theta)_i \frac{\nabla_{x_i} p(x)}{p(x)} dx \tag{1.}\\
&= \int \sum_i s(x, \theta)_i \nabla_{x_i} p(x) dx \\
&= \int \sum_i \nabla_{x_i} s(x, \theta)  p(x) dx \tag{2.}\\
&= \int p(x) \nabla_x \cdot s(x, \theta) dx \\
\end{align*}
$$


$$
\begin{align*}
\nabla_x \log p(x) &= \frac{\nabla_x p(x)}{p(x)} \tag{1.} \\
\int \nabla_x f(x) \cdot g(x)dx &= \int f(x) \cdot\nabla_x g(x)dx \tag{2.}\\
\end{align*}
$$

Thus we have:

$$
\mathcal L_{SM}(\theta) = \mathop{\mathbb E}_{x\sim p(x)} \Big[ \parallel s(x, \theta) \parallel^2  + 2 \nabla_x \cdot s(x, \theta)\Big]
$$

<!-- ***

Quick aside, what is the divergence?


$$
\begin{align*}
\Delta f &= \nabla \cdot \nabla f = \nabla^2 f \\
\Delta \log p(x) &= \nabla \cdot \nabla \log p(x) = \nabla^2 \log p(x) \\
&\approx \nabla \cdot s(x, \theta) \\
\end{align*}
$$

-->

Let's try and understand what this objective doing.

<aside>
What is divergence?

The divergence of a vector field is a scalar field that measures the rate at which the vector field diverges from or converges to a point.

<img src="{{ site.baseurl }}/assets/scoredd/Divergence.svg" class="center-image" alt="The score function should be a sink at the data points.">

</aside>

The objective is trying to minimise;

- The norm of the score function. This is a regularisation term that prevents the score function from becoming too large.
- The divergence of the score function. This term ensures that the score function is a sink at the data points.

Points with higher density should be larger sinks (to minimise the divergence).

<img src="{{ site.baseurl }}/assets/scoredd/score-matching.png" class="center-image">
<figcaption>At a given point, the score function should be a sink proportional to the density of the data at that point. But the sinks interfere with each other. Letf figure shows sinks drawn everywhere. Right figure shows the sinks after they interfere with each other.</figcaption>

<!-- Relationship between the 2-norm of the score and the gaussian dist? -->

<!-- TODO to derive this score matching objective for other distributions?
While would yield pairings of distribution and norm? -->

<!-- ineresting part is that this recovers scores that point in the right direction with right magnitude for gaussian noise.
what if we use a different norm / metric? this will yield a non added gaussian noise score fn.!? -->

### Denoising score matching

Denoising score matching,  {% cite vincent_connection_2011 Alain2012 %} starts by slightly changing the goal. Instead of learning the score function of the data distribution, we learn the score function of a parzen-window density of the empirical data we observe.

$$
\begin{align*}
q(\hat x) &= \int_x p(\hat x \mid x) p(x) dx \\
&= \int_x \mathcal N(\hat x \mid x, \sigma^2I) p(x) dx \\
&\approx \frac{1}{N}\sum_i^N \mathcal N(x \mid x_i, \sigma^2I) \tag{$x_i\sim p(x)$}\\
\end{align*}
$$

Our DSM objective function becomes:

$$
\begin{align*}
\mathcal L(\theta) = \mathop{\mathbb E}_{x\sim p(x)} \mathop{\mathbb E}_{\hat x \sim p(\hat x\mid x)} \parallel s(\hat x, \theta) - \nabla_{\hat x} \log q(\hat x) \parallel^2 \\
\end{align*}
$$

<!-- ***
why would we want to learn the score of this noise distribution?
Now, how are these two distributions related? What does $\nabla_x \log q(x)$ tell us about $\nabla_x \log p(x)$?

want to know. do they always point the same direction?

for a finite sample, how wrong are we likely to be? p(x) - p_{\theta}(x)

TODO prove...

$$
\lim_{\sigma \to 0} \nabla_{\hat x} \log q_{\sigma}(\hat x \mid x) = \nabla_{x} \log p(x) \\
$$
This is wrong?!?
$$
\lim_{n \to \infty} \nabla_{\hat x} \log q_{\sigma}(\hat x \mid D_n) = \nabla_{x} \log p(x) \\
$$
also. wrong. need sigma to be small!?

Also prove that for large datasets, the difference is small. (assuming some kind of smoothness on $p(x)$)

$$
 p(x) \approx q_{\sigma}(\hat x \mid D_n) \\
q_{\sigma}(\hat x \mid D_n) = \frac{1}{n} \sum_{i=1}^n \mathcal N(\hat x \mid x_i, \sigma) \\
\epsilon = \text{KL}(p(x) \parallel q_{\sigma}(\hat x \mid D_n)) \\
\epsilon < f(n) \\
$$

*** -->

<!-- Want some pics?! 

- $q(\hat x \mid x)$ tells us a lot about $p(x)$.
- score fn arrows pointing towards the x's. -->
Since we have chosen our noise distribution to be gaussian, we can calculate the score function of this distribution.

$$
\begin{align*}
\nabla_{\hat x}\log q(\hat x | x, \sigma) &= -\frac{1}{\sigma^2} (\hat x - x) \\
p(\hat x \mid x, \sigma) &= \mathcal N(\hat x \mid x, \sigma^2I) \\
\nabla_{\hat x}\log \mathcal N(\hat x \mid x, \sigma^2I) &= -\frac{1}{\sigma^2} (\hat x - x)
\end{align*}
$$


Thus our loss becomes:

$$
\begin{align*}
\mathcal L(\theta) &= \mathop{\mathbb E}_{x\sim p(x)} \mathop{\mathbb E}_{\hat x \sim p(\hat x\mid x)} \parallel s(\hat x, \theta) + \frac{1}{\sigma^2} (\hat x - x) \parallel^2 \\
\end{align*}
$$

Where the model $s(\hat x, \theta)$ attempts to predict the (normalised) noise added to $\hat x$. Aka, the score function (for additive gaussian noise).

Hopefully the connection to denoisers is clear. 
Where a denoiser would predict $x$ given $\hat x$, we are predicting the noise added to $\hat x$ given $x$.
So it's possible to extract the score function from a denoiser via 

$$
s(\hat x, \theta) = \frac{1}{\sigma^2} (\hat x - D(\hat x, \theta))
$$

<!-- Tweedie's formula
$$
\mathbb E[x\mid \hat x] = \hat x + \sigma^2 \nabla_{\hat x} \log p(\hat x)
$$

[@meng_estimating_nodate]  -->



<!-- Given the structure of this solution. How about parameterising $f(\hat x, \theta) = \hat x + \sigma^2 \nabla_{\hat x} \log f(\hat x, \theta)$. Well some clever people already thought of that. -->

<!-- #### Experiments

Score matching vs denoising score matching. Which is better? -->

<!-- how about in higher dimensions!? divergence would be more reliable than denoising? -->

<!-- try both losses. which learns faster? -->
<!-- Would rather prove which has lower variance when estimated from a finite sample. Or its convergence speed -->

<!-- ***

Why DSM?

> One way to find an estimate θ for the energy E(x; θ) would be to simply
apply the basic definition of score matching. However, as discussed above, such an approach is often
considered to be computationally prohibitive due to the appearance of second-order derivatives in
the objective function (Kingma & LeCun, 2010) and one must resort to approximations (Martens
et al., 2012a). Our key insight here is to build on the Bayesian interpretation of the score function of
Raphan & Simoncelli and the Parzen score matching of Vincent. This leads to a method which scales
well to high dimensions, as well as to large datasets

https://arxiv.org/pdf/1805.08306.pdf -->


<!-- ## Connection between max likelihood and score matching

Instead of the KL divergence, define the fisher divergence.

$$
D_F(p, q) = \mathbb E_{x \sim p(x)} \mid \frac{\nabla_x p(x)}{p(x)} -  \frac{\nabla_x p_{\theta}(x)}{p_{\theta}(x)} \mid^2 \\
$$


$$
D_{KL}(p, q) = \mathbb E_{x \sim p(x)} \log \frac{p(x)}{p_{\theta}(x)} \\
D_{F}(p, q) = \mathbb E_{x \sim p(x)} \mid \nabla_x \log \frac{p(x)}{p_{\theta}(x)} \mid^2 \\
$$

$$
y(t) = x + \sqrt{t} w \\
\frac{d}{dt} D_{KL}(p(y(t), t), p_{\theta}(y(t), t)) = - \frac{1}{2} D_F(p(y(t), t), p_{\theta}(y(t), t)) \\
$$ -->

<!-- [@lyu_interpretation_2009] -->

<!-- ## Learning the score fn

So far we have covered the what and how of score matching. But we haven't covered the why. 

Why does it work? Is it fundamentally easier to learn the score than the density? -->

<!-- [@pabbaraju_provable_2023] -->

## Diffusion

> What does all this have to do with diffusion?

<!-- quick reminder what diffusion is -->
In diffusion we construct a stochastic differential equation (SDE) that describes the evolution of a distribution over time. {% cite song_score-based_2021 %}

$$
\begin{align*}
dx &= f(x, t)dt + g(t)dW_t \tag{forward SDE}\\
dx &= [f(x, t) - g^2(t) \nabla_x \log p(x)_t] dt + g(t)dW_t \tag{reverse SDE}\\ 
\end{align*}
$$

The learning objective in to approximate the score, so we can reverse the process.

***

When discretised, we can view the forward process as

$$
p(x_{t+1} \mid x_t) = \mathcal N(x_{t+1} \mid x_t, \sigma(t)^2I) \\
x_{t+1} = x_t + \sigma(t) \epsilon_t \\
$$

So, to approximate the score function we can use denoising score matching.
Undoing the repeated addition of noise.

So we stack many denoising score matching models on top of each other. Each model is trained to denoise the output of the previous model.


<img src="{{ site.baseurl }}/assets/scoredd/denoising-diffusion.png" class="center-image" alt="">
<figcaption>From left to right we have the addition of increasing amounts of Gaussian noise. Learning to invert the added noise between two neighbors can be solved via denoising score matching.</figcaption>

Using this stack of denoisers, we can generate new samples. By taking a sample from a gaussian distribution (right) and passing it through each denoiser. This procedure turns out to be equivalent to doing Langevin dynamics using the score function.

This is cool! Stacked denoisers are much easier to understand than score functions, Langevin dynamics, and SDEs.

<!-- prove this!! -->


## Bibliography

{% bibliography -f scoredd --cited %}