---
title: "Diffusion posterior sampling"
subtitle: "A review of recent work"
layout: post
permalink: /pits/review-dps
categories: 
  - "tutorial"
scholar:
  bibliography: "pits.bib"
---

(_lit review as of 07/2024_)

Given a pretrained diffusion model, we seek to generate conditional samples based on an observed signal $y$.
For example, we many be given a noisy image and tasked with denoising it, or a black and white image and tasked with recoloring it.

One approach seeks to augment the dynamics of the pretrained diffusion model. We call these guided diffusion models, after 'guided diffusion' {% cite ho2022classifierfreediffusionguidance %}.

Early approaches were rather heuristic, for example; a mask-based approach {% cite lugmayr_repaint_2022 %}, SVD inspired {% cite kawar_denoising_2022 %}, null space projection {% cite wang_zero-shot_2022 %}.

Next came diffusion posterior sampling (DPS), a more principled approach. It starts by rewriting the diffusion SDE to use the unknown posterior score, $\nabla_x \log p(x \mid y)$, rather than the prior score, $\nabla_x \log p(x)$.

$$
\begin{align*}
    dx &= \left[ f(x, t) - g(t)^2 \nabla_x \log p_t(x) \right] dt + g(t) dw \tag{unconditional SDE} \\
    dx &= \left[ f(x, t) - g(t)^2 \nabla_x \log p_t(x | y) \right] dt + g(t) dw \tag{conditional SDE} \\
\end{align*}
$$


This allows us to generate samples from the posterior by solving the conditional SDE.

But, we don't know the score of the posterior, so we use Bayes' rule to rewrite the posterior score in terms of the likelihood and prior scores.

$$
\begin{align*}
    p(x | y) &= \frac{p(y | x) p(x)}{p(y)} \\
    \log p(x | y) &= \log p(y | x) + \log p(x) - \log p(y) \\
    \nabla_x \log p(x | y) &= \nabla_x \log p(y | x) + \nabla_x \log p(x) \\
\end{align*}
$$

DPS {% cite chung_diffusion_2023 %}, $\Pi$GDM {% cite song_pseudoinverse-guided_2023 %} and others have shown that it is possible to construct / approximate $\nabla_x \log p_t(x \mid y)$.
Note that $\Pi$GDM has also been applied to flows {% cite pokle_training-free_2024 %}.

$$
\begin{align*}
    \nabla_x \log p(y | x_t) &\approx \nabla_x \parallel y - C(x) \parallel^2_2 \tag{DPS} \\
    \nabla_x \log p(y | x_t) &\approx (y - H\hat x)^T (r_t^2 H H^T + \sigma^2I)^{-1} H \frac{\partial \hat x_t}{\partial x_t} \tag{$\Pi$GDM}
\end{align*}
$$

In parallel, a variational approach frames the conditional generation problem as an optimization problem {% cite benhamu2024dflowdifferentiatingflowscontrolled mardani_variational_2023 mardani_variational_2023-1 %}. 

$$
\begin{align*}
    x^* &= \arg \min_z \nabla_z \parallel y - f(D(z)) \parallel^2_2 \tag{DPS} \\
\end{align*}
$$

Where $D$ is the diffusion model, $f$ is the forward model, $z$ is the latent variable and $y$ is the observed signal.
These variational approaches proidve high quality samples, but are computationally expensive (approx 5-15 minutes for ImageNet-128 with an NVidia V100 GPU).


And finally {% cite dou_diffusion_2024 %} present a Bayesian filtering perspective which leads to an algorithms that converges to the true posterior.


## Bibliography
{% bibliography --cited %}