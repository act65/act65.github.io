---
title: "ML and Education"
date: "2016-12-30"
layout: post
---

![]({{site.baseurl}}/images/{{page.coverImage}})

### Dropout (not that sort...).

Add noise to the learning environment so that what you learn is more robust to changes in the outside world. I guess this is the classic air horn while trying to shoot hoops. But it also relates to adding constraints as dropout forces you to use less resources. Imagine trying to build a house, but you are told that today you are not allowed to use your hammer. You must be creative and find another way. Thus encouraging robust knowledge.

### Adversarial learning.

When studying for your next exam, imagine that you and your lecturer were not on good terms. Maybe you had been, skipping all classes, ... dating their child, or ...? So the setting becomes: the lecturer wants you to fail, and they are going to design the exam accordingly. What would you study?

### Practice makes perfect.

Current DL algorithms require thousands of examples. When trying to achieve 99% accuracy rather 98% it can take millions of examples to learn this difference.

### Auto-encoders and generative models

If you can reconstruct and/or generate the input (the argument, or proof, or program, or ...) then you have probably learned something useful about the task at hand.
