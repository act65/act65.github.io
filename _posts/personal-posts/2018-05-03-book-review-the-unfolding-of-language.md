---
title: "Book review: The unfolding of language"
date: "2018-05-03"
coverImage: "unfolding.jpg"
layout: post
---

![]({{site.baseurl}}/images/{{page.coverImage}})

Languages mutate in predictable ways;

- the structure and sound of words is changed to save energy
- the meaning of words is dulled over time
- metaphors are abstracted
- learners generalize spurious patterns.

### Erosion

Some of the mutations tend to make a language less elegant, more complex, ... but why and what are they?

#### Energy saving shortcuts

Some words are harder to say than others, try pronouncing "_fhtrskvz_"... Common ways English speakers have cheated are;

- Adding vowels: Too many consonants together.
- Reordering hard to pronounce consonants: `fihs -> fish`
- Swapping some consonants for similar, softer ones, see [Grimm's law.](https://en.wikipedia.org/wiki/Grimm%27s_law)
- Merging vowels: `ae -> i`.
- Flower (latin): Flos (nominative), flosem (accusative), flosis (genitive), flosi (dative), flose (ablative). `flosis` changed to `floris` because it is easier to pronounce without getting tongue-tied.

Questions

- Why do some languages lose vowels while others lose consonants? How do vowels and consonants relate to the sounds we make?
- What about energy saving changes at other levels; morphology and/or syntax!?
- Supposedly d->t (according to Grimm's laws), but is it really true that d takes more energy to pronounce than t?

#### Diminishing distinctiveness

> "_the strength of a meaning of a particular word depends on its distinctiveness_"

Catastrophe is currently used to mean something kinda bad. And literally is often used to mean figuratively. It might seem silly, but using these more extreme words give us the ability to draw attention.

Eventually, these hyperbolas will become less distinct and catastrophe will be used to mean bad, and bad will not be used at all, and a new word will be invented to mean catastrophe.

### Sedimentation (the opposite of erosion)

Ok, the erosion of languages via energy saving shortcuts and the diminishing distinctiveness of words makes sense to me, but where does the order that is being eroded come from? (this is the main question addressed by the book)

#### Abstraction

Once upon a time, have used to mean grasp/hold (and it some ways it still does). But it was consistently used in a metaphor for hold more abstract things. I hold (have) a thought.

(Thus a metaphor IS an abstraction!?)

`metaphor/abstraction -> cliche/independent unit (-> erosion) -> function word`

(not sure if this is the same type of abstraction as what the book as on about. found this a little unclear)

The interesting part is that the new function word inherits the syntax of the of metaphor. Thus the grammar was never invented, but rather imposed by how/when they phrase was used. (!!!)

#### The order-craving mind

By chance we may have two irregular tense forms where the past is indicated with an added "-ed" in both cases. When a child is learning the language, they might be tempted to generalise this past tense marker, "-ed" , to other words, for example, "I runed" (rather than I ran).

But how does this generalisation make its way from the child's mouth into a language?

- At what rate does erosion act compared to the accrual of order?
- What are the limits of a generalisation made by an individual? (how large, how much compression, ...?)
- Which generalisations tend to be accepted and why?

When learning a language it is easier to start with general patterns in word structure, morphology, suffixes, grammar, ... and then ... One of our built in biases. (!!!)

* * *

After this section I was left with many questions;

Why have languages been loosing morphological complexity and gaining syntactic complexity? Is it really true that all languages have been loosing morphological complexity? Or just the ones we considered? How is syntactic complexity better than phonetic/morphological complexity? Over time, is complexity distilled from morphology -> syntax!? Once compositional syntax has evolved, is the need for complex morphology reduced?

How does syntax evolve? We were given a process by which new rules can be generated (generalization of something concrete), but how do existing rules change with the generations and do syntactic rules erode?

### Evolution of a language

Guy composes a series of steps together to give a candidate trajectory that languages would/could/should take in their evolution.

​`Action words x item words x "Natural principles" -> Pointing -> More specific pointing -> Noun/verb -> More specific actions -> Relative clauses -> ?`

### "Natural principles" (of grammar)

Monsieur Jourdain's principle

"_things that belong together in reality ought to appear side by side in language_" This seems like an oversimplification as things can belong together in many different ways depending on your perspective. Aka locality.

Cesar's principle

I came, I saw, I conquered. "_the order in which events are expressed in language mirrors the order in which the occur in reality_"

Don't be a bore

We don't need to refer to an entity many times. Effectively asserting that entities can be resolved given context.

* * *

If these laws are so natural, then why does language evolve to break them? Not convinced!

### Specificity

We need to index entities/things/objects in the real world. How can this be done efficiently? Clarity is expensive, and only necessary if the index is not 'obvious'.

"_Cat!_" But which cat???

- Pointers!?: _This cat_ (here/there, ...)
- Pre/post-positions: _Cat in hat__._ (Ontop of, near, behind, ... )
- Property-words: T_abby cat in red hat_. (adjectives)
- Pronouns: _My cat in your hat._ (he, she, it, him, herself, me, we ... )
- Possessives: _My cat in Tabitha's hat_ (of, \_'s, ... )
- Quantifiers: _Every cat in any hat_ (all, whole, ... ) reminds me of set theory
- Articles: _The cat in a hat_ (a, one, ... )

We seem remarkably good at guessing the right entity given partial information.

### Noun/verb duality

Action - thing distinction. An object is always a noun, but a noun is not always an object, for example "day". Guy assumes that the human brain naturally distinguished actions and objects (a testable hypothesis).

More interestingly, he goes on to note the asymmetry in the translation of nouns to verbs and vice versa.

- `nominalisation: V -> N` "move -> movement", "explode -> explosion", "walk -> walking"
- `verbalisation: N -> V` "to water", "to cage", ...

Supposedly the extra difficulty we take to turn verbs into nouns reflects some deep structure in how people see the world.

### The structure of our minds

Food for thought. What does our brain do that seems interesting, important, powerful, necessary, unnecessary?

- Reference frames, pointing and taking different perspectives.
- Agent/actor centered view of the world.
- "Order craving mind". Generalise first ask questions later.
- Metaphor!
- Action - object distinction. Nouns capture more than just objects, yet, ... Nouns inherited the structure of objects (permanence, translation, ...?)

### Irregular verbs

Irregular verbs are interesting! Our desire to save energy is stronger than our need to compress the knowledge (at least at an individual level). Is this effect produced by how languages are mutated? Once learned by an individual, a language is optimised for saving energy, this new energy efficient language is then taught to the next generation, who optimise it for compression (while they learn).

### Thoughts

- Guy's derivation of the root structure of Semitic languages was incomplete/not convincing. Only proves sufficiency, not necessity. Starts from a complex case system (how was that evolved?).
- Preturb (optimise easy of communication - energy, meaning) and compress (optimise total info, ease of learning).
- What about the optimisation of words for distinguishability. The inspiration of the sound words by what they are labeling.
- Trade-off between the ability to express ideas, pronounce and distinguish spoken words,
- Verbs as functions that take arguments (nouns)!
- Interestingly, there is no mention of a MERGE operation.
