---
title: Beyond Earning to Give
subtitle: The EA Road to AI
layout: post
---

The common caricature of Effective Altruism (EA) is a community of calculating rationalists trying to boil down human compassion into a spreadsheet. This picture isn't entirely wrong, but it mistakes the tools for the purpose. The movement's origin lies not in a spreadsheet, but in a simple, powerful moral question.

Philosopher Peter Singer famously posed a thought experiment: you see a child drowning in a shallow pond. Saving them is easy, but it will ruin your expensive new suit. Do you do it? The answer is obvious—a child's life is worth more than a suit. Singer's point is that we are in a similar situation every day. Children in distant countries die from preventable diseases, and we could save them at a relatively small cost to ourselves. This is the moral heart of Effective Altruism: if we can help others, we ought to. [^1]

This question first gained serious traction in the 2000s, leading to an emphasis on global health charities where the impact of a dollar could be rigorously measured. This is the "effective" part: a commitment to using evidence and reason to guide our altruistic efforts.

### From Philosophy to Practice: Earning to Give

The initial, practical answer to Singer's challenge was a strategy called "Earning to Give." Popularized by organizations like 80,000 Hours, the idea is straightforward: if you want to fund life-saving interventions, you should pursue a career that allows you to earn a lot of money and donate a significant portion of it to the most effective charities. [^2] For a time, this was seen as one of the highest-impact paths, a direct and measurable way to turn your career into saved lives.

But this is only half the story. The initial focus on measurable outcomes naturally leads to a deeper, more complex question that cracks the caricature wide open: What about the unquantifiable? What is the value of discovering calculus or inventing the transistor? These weren't obviously useful at the time, yet their long-term impact on human well-being has been astronomical. This question reveals the true intellectual heart of modern EA—a rigorous attempt to make progress in the face of profound uncertainty.

### The Impossibility of Precise Calculation

The first thing a modern EA practitioner should admit is that calculating a precise expected value for long-term, speculative projects is impossible. Our dim minds are susceptible to motivated reasoning and confirmation bias. Anyone who claims to have a precise dollar value for foundational mathematics research is fooling themselves.

So why use an optimization framework at all? Because its true purpose is not to provide a definitive answer, but to **structure our thinking and combat our biases**. It forces us to articulate our assumptions, beliefs, and goals.

Let's take a simplified, worked example. Imagine choosing between two paths:
1.  **Path A:** Becoming a doctor in a developed country.
2.  **Path B:** Researching a novel malaria vaccine.

A rigid, calculative approach would fail. The EA framework, however, forces you to ask the right questions to determine your **marginal impact**—the net positive contribution you make beyond what would have happened anyway. Key considerations include:

*   **Importance/Scale:** How much good is done if the problem is solved? As a doctor, you will certainly help many people. A successful vaccine, however, could help millions, making the scale immense.
*   **Tractability:** What are the chances of making a meaningful contribution? The path to becoming a doctor is well-defined. The path to a new vaccine is fraught with failure.
*   **Neglectedness:** How many other talented people are working on this? Many people are becoming doctors, so if you don't take a spot, someone nearly as qualified probably will. Your *marginal impact* might be relatively small. Conversely, if the specific vaccine research area is underfunded and overlooked, your contribution could be pivotal. Neglectedness is often the strongest indicator of high marginal impact. [^3]

This process doesn't spit out a number. Instead, it illuminates the trade-offs and forces an honest comparison, steering you away from choices based purely on what feels prestigious or familiar.

### From Total to Partial Ordering

It's tempting to think we can rank all possible good deeds from best to worst—a **total ordering**. This is the spreadsheet caricature. But it's impossible to definitively say if curing cancer is "better" than creating a unified model of physics.

A more realistic approach is to create a **partial ordering**. We can't rank everything, but we can identify relationships. We can say that some actions are clearly more impactful than others. We might not be able to compare curing cancer and advancing physics, but we can be reasonably confident that both are more impactful than, say, redesigning a luxury yacht.

This mindset led the community to diversify its efforts. It began by **exploiting** known, high-return opportunities, like funding insecticide-treated bed nets. But a robust strategy must also **explore** high-risk, high-reward research. Many of these projects will fail, but the few that succeed could change the world immeasurably.

### The Logical Path to AI

This framework of searching for upstream, high-leverage opportunities leads directly to the movement's focus on artificial intelligence. It’s not an arbitrary obsession, but the logical conclusion of a simple, powerful heuristic:

*If technology X can be used to solve problems A, B, and C, then working on X is a higher-leverage activity than working on A, B, or C individually.*

Consider this chain of reasoning:
1.  A new drug could cure a specific disease.
2.  A quantum chemistry simulation tool could help design *many* new drugs for many diseases.
3.  A breakthrough in general machine intelligence could accelerate progress on *everything*, including developing better quantum chemistry simulations.

We can see a similar pattern in other areas. Tackling human trafficking, the arms trade, and political corruption are all vital. However, these problems are all facilitated by illicit international financial flows. Solving the problem of illicit finance could therefore be a more "upstream" and higher-leverage intervention.

This line of reasoning is why AI safety is not just another cause for EA; it is seen as potentially the *most* upstream cause. A beneficial, aligned AGI could help solve nearly every other problem we face. Conversely, a misaligned or unsafe AGI could render all other progress irrelevant. It is the ultimate high-stakes, high-leverage, and—for now—critically neglected problem. It is the destination that all roads of effective, altruistic reasoning seem to lead to.

---
[^1]: Singer, Peter. "Famine, Affluence, and Morality." *Philosophy & Public Affairs*, vol. 1, no. 3, 1972, pp. 229–43.
[^2]: MacAskill, William. "Earning to Give." *80,000 Hours*, 2012, https://80000hours.org/articles/earning-to-give/.
[^3]: MacAskill, William. *Doing Good Better: How Effective Altruism Can Help You Make a Difference*. Guardian Books, 2015.