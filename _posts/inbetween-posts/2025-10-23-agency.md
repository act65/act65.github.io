---
title: Is democracy the optimal algorithm for collective freedom?
subtitle: Connecting agency, universal intelligence, and mechanism design for a 'free' society.
layout: post
categories:
    - economic
---

## A Formal Framework for Freedom

I’ve been thinking about the relationship between freedom and democracy, inspired by Peter Thiel’s claim that they are no longer compatible. Rather than debating the statement directly, I'm interested in building a framework from first principles.

The goal is to start with a rigorous, mathematical definition of what enables a society's members to achieve their goals. I will call this concept **agency**. While "freedom" is a broad philosophical term, often connoting "freedom from" interference, agency is a more specific, measurable concept of "freedom to" act effectively. From a formal definition of agency, we can then ask what kind of social structure would be optimal for maximizing it - democracy?

### The Search for a Formal Definition, Illustrated with Commuters

To make this concrete, let's use a simple multi-agent universe: a city grid with commuters. We will start by trying to define an individual's agency.

**Attempt 1: Agency as Immediate Options**

> tldr: count the current actions available

A simple start is to define a commuter's agency as the number of roads available from their current position. If they are at a four-way intersection, their agency is 4; on a one-way street, it's 1. This is intuitive but myopic; it fails to capture any sense of long-term potential. A road leading to a dead-end is not as valuable as one leading to an open highway.

**Attempt 2: Agency as Future Trajectories**

> tldr: count the possible unique futures

A better model is to consider a commuter's potential futures. We could define their agency as the total number of unique paths they could take from their current position. For a commuter on a 10x10 grid, this would be a vast number of possible routes.

However, this has a critical flaw: **it treats all futures as equally plausible, ignoring the inherent probabilities of the environment.** Imagine some roads are perfectly paved highways, while others are treacherous gravel paths with a 50% chance of causing a flat tire on any given block. A simple count would treat a 10-mile path on the highway and a 10-mile path on the gravel as equivalent futures. But in reality, the gravel path is far less likely to be completed successfully. The model counts all theoretical possibilities without grounding them in the probabilistic nature of the world.

**Attempt 3: Agency as Probabilistic Futures**

> tldr: weight possible futures by their likelihood

To fix this, we can weight each future path by its probability of successful completion. The 10-mile highway path might have a 99.9% probability, while the 10-mile gravel path's probability would be astronomically lower. This model might correctly predict a commuter is most likely to end up near a highway exit.

This is a significant improvement, as it reflects real-world constraints. However, the definition is still passive. It doesn't capture the commuter's *intent*. What if their goal is to get to a remote cabin only accessible by the gravel road? The model measures likely futures, not the **power to override the likely future** to achieve a specific goal.

**Attempt 4: Agency as Achievable Goals**

> tldr: measure the number of goals achievable

This leads to our most sophisticated individual-level definition. Let's define agency as **the size of the set of goals an individual can achieve.** This is closely related to the concept of **controllability** in control theory—the ability to steer the future toward a desired objective.

This definition is powerful because it is active, forward-looking, and purposeful. And yet, it fails for the most fundamental reason: **an individual's set of achievable goals is not an independent property of that individual.**

Consider the goal: "Park in the last open spot at the destination." Your ability to achieve this goal—your control over this outcome—is entirely contingent on whether another commuter shares the same goal. Your agency can be created or destroyed purely by a change in someone else's intent.

This is the core insight. Because an individual's agency is so fundamentally entangled with the goals and strategies of the entire system, any attempt to calculate it as a single, isolated number is a futile exercise. The object of measurement is wrong.

This forces our conclusion: a meaningful definition of agency cannot be a score assigned to an individual. It must be a measure of the **entire system's capacity** to handle the interacting, and often conflicting, goals of all its members.

### A More Robust Definition: Agency as a System's Capacity

This leads us to a final, more holistic definition. Instead of trying to calculate an isolated score for each individual, we will define a single agency score for the entire social system. This score measures the system's overall capacity to enable its members to achieve their goals.

The logic is as follows:
1.  First, we imagine a "joint goal vector," $\gamma = (g_1, g_2, ..., g_n)$. This is a specific combination of goals, one for each person in the society (e.g., commuter 1 wants to get to the hospital, commuter 2 to the airport, etc.). We will "stress test" our society by seeing how well it can satisfy this vector.
2.  For this given goal vector, the $N$ individuals play a non-cooperative game. Each person $i$ acts independently, choosing a strategy $\sigma_i$ to maximize the probability of achieving *their own goal* $g_i$.
3.  This is not a simple optimization problem. The stable outcome of these simultaneous, interacting choices is a **Nash Equilibrium**. This is a strategy profile $\sigma^*(\gamma)$ where no one can improve their outcome by unilaterally changing their strategy. It's the point where everyone is doing the best they can, given what everyone else is doing.
4.  At this equilibrium, we can define a "success score" for this joint goal. We define success as an outcome where everyone succeeds, which mathematically is the *product* of their individual success probabilities: $\prod_i P(g_i \mid \sigma^*(\gamma))$.
5.  Finally, the total agency of the system, $K_{system}$, is the sum of these success scores over *every possible joint goal vector* the society could ever face.

This gives us a measure of the society's general, goal-achieving power.

### The Formal Definition

The total agency of a social system is:

$$ K_{system} = \sum_{\gamma \in G_{joint}} \left[ \prod_i P(g_i \mid s_t, \sigma^{* }(\gamma)) \right] $$

Where $\sigma^{* }(\gamma)$ is the Nash Equilibrium strategy profile that satisfies the following system of $N$ conditions: for every player $i$, their strategy $\sigma_i^{* }$ in the profile $\sigma^{* }(\gamma)$ must be the solution to their personal optimization problem:

$$ \sigma_i^* = \underset{\sigma_i}{\mathop{\text{argmax}}} P(g_i \mid s_t, \sigma_i, \sigma_{-i}^*) $$

### A Parallel in Artificial Intelligence

Interestingly, this definition of agency has a strong parallel to formal definitions of intelligence in the AI community. One of the most well-known is the Legg-Hutter model for a universal intelligence:[^1]

$$ \Upsilon(\sigma) = \sum_{\mu \in E} 2^{-K(\mu)} V(\mu, \sigma) $$

The parallel is clear. Both our $K_{system}$ and their $\Upsilon$ are summing up an agent's (or a system's) performance over a vast space of possible goals or environments. We have defined freedom as a measure of a society's **collective problem-solving capacity**—its ability to empower its citizens to achieve their chosen objectives.

### Embedding Fairness in the Definition

The use of the product operator $\prod_i P(g_i)$ inside the sum is a crucial choice. It defines a "successful outcome" for any given goal vector as one of mutual success. A sum would have valued a scenario where half the people succeed wildly and half fail completely. The product, however, is maximized when success is distributed as evenly as possible. This formulation is known as the Nash Social Welfare Function, and it is a common choice in the formal study of social choice.[^2][^3] An outcome where even one person's probability of success $P(g_i)$ is zero results in a score of zero for that entire joint goal. This embeds a principle of non-domination and a preference for egalitarian outcomes directly into our measure of societal success.

### On the Equality of Goals

This framework is neutral about which goals people should have. The sum $\sum_{\gamma \in G_{joint}}$ treats every combination of human aspiration as equally worthy of consideration.

However, the *consequences* of these goals are not treated equally by the system's dynamics. This influence is captured in the Nash Equilibrium $\sigma^*(\gamma)$. Your goal to become a doctor creates positive externalities that alter the equilibrium, making it easier for others to achieve their goals (e.g., "survive an illness"). Conversely, consider an individual whose goal is to "get to work as fast as possible by driving recklessly." Their actions would create negative externalities, drastically lowering the success probabilities $P(g_i)$ for countless other commuters. Our $K_{system}$ formula inherently penalizes societies that allow such destructive goals to destabilize the system and harm others.

### Next Steps: Designing the Game of Society

It is important to be clear: the purpose of this framework is not to calculate a $K_{system}$ score for any real-world society. Such a calculation would be computationally impossible, requiring us to sum over an infinite set of goals and find equilibria for games with billions of players.

Rather, $K_{system}$ is a **conceptual objective function**. Its value is in allowing us to study small, toy systems and ask formal questions. By designing mechanisms for these toy systems and observing which ones maximize $K_{system}$, we can gain insight into the properties of good societal rules. Do K-maximizing mechanisms look like democracy? Do they protect individual rights? These are the questions this framework allows us to formally investigate.

The challenge of political philosophy can therefore be framed as a problem in **mechanism design**—the art of designing the rules of a game to achieve a desired outcome. We do this by designing the mechanism, $m$—the fundamental rules of the game (a constitution, laws, property rights). The mechanism doesn't determine the outcome directly; it **induces a game**. By changing the rules $m$, we change the payoff structure and available actions, which in turn changes the resulting Nash Equilibrium $\sigma^{* }(\gamma, m)$ and the associated success probabilities $P(g_i \mid ...)$.

The optimization problem is therefore:

$$ m^* = \underset{m}{\text{argmax}} [ K_{system}(m) ] $$

In plain English: The best society ($m^*$) is the one whose fundamental rules ($\text{argmax}_m$) create a game where the stable pattern of behavior that emerges from everyone independently pursuing their own goals results in the highest possible capacity for collective success, across all possible futures.

This gives us a formal basis for the next stage of this project: to analyze democracy not as a moral axiom, but as a candidate mechanism, and to test its ability to solve this problem.

---
[^1]: Legg, S., & Hutter, M. (2007). *Universal Intelligence: A Definition of Machine Intelligence*. Minds and Machines, 17(4), 391-444.
[^2]: The formal study of social welfare functions was pioneered by Kenneth Arrow in his 1951 book *Social Choice and Individual Values*.
[^3]: This formulation is known as the Nash Social Welfare Function, derived from the principles in John Nash's 1950 paper, *The Bargaining Problem*.