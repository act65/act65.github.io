---
layout: post
title: Language models are all you need (for SMILES-based chemistry)
permalink: lm-chem
---

> Let's train a single large language model on for as many different chemistry tasks as possible.

<side>
This post was adapted from a presentation.
</side>

Let's imagine a few tasks;

### Property prediction

For examples, tasks could be;
- "The logP of C1=CC=NC=C1 is " __->__ "0.65"
- "The boiling point of CC(C)Cc1ccc(C(C)C(=O)O)cc1 at 1atm is" __->__ "157 C"
- any / all physical properties. (molecular weight, melting point, pKa, NMR shifts, refractive index, ... )

There are many sources of data for these tasks, for example; [Reaxys](https://www.reaxys.com/#/search/quick), [Pubchem](https://pubchem.ncbi.nlm.nih.gov/).


### (graph) structure elucidation

We could prompt the LLM with;
- Prompt: _"Molecular formula: C4H6O4. Observed 13C NMR peaks: 28.9, 173.9. Elucidated structure: \_\_\_\_\_\_\_\_\_\_"_<br>
- Completion: _"C(CC(=O)O)C(=O)O"_

Or;
- _"Molecular formula: C21H22N2O2. Observed 13C NMR peaks: 1.22, 1.41, 1.83, 1.84, ... Elucidated structure: \_\_\_\_\_\_\_\_\_\_"_<br>
- Completion: _"O=C1CC2OCC=C3C4C2C2N1c1ccccc1C12CCN(C1C4)C3"_

We could use data from [NMRShiftDB](https://nmrshiftdb.nmr.uni-koeln.de/) for this task.

### Translation from natural language to synthetic 'program'

The input prompt could be;

- Prompt: _"Text: A vial was charged with LiCl (0.32 g) in anhydrous THF (15 mL). Program: \_\_\_\_\_\_\_\_\_\_"_ <br>
- Completion: _"\<AddSolid vessel="reactor" reagent="LiCl" mass="0.32 g" \/\>\<Add vessel="reactor" reagent="THF" volume="15 mL" \/\>"_

A dataset for this could be sourced from [orgsyn](http://www.orgsyn.org/) or [Pistachio_2017](https://www.nextmovesoftware.com/pistachio.html).

***

> All of chemistry in a LM.

What are other tasks we could include?

- reactivity?
- drug design?
- retrosynthesis?

How to include other types of chemical data?

- [QM9](http://quantum-machine.org/datasets/) (smiles, 3D positions, electron densities)
- [Open catalyst project](https://opencatalystproject.org/) (trajectories, forces and energies)
- Theoretical knowledge. Like the dataset used [here](https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/6393827c836cebbc757aedeb/original/assessment-of-chemistry-knowledge-in-large-language-models-that-generate-code.pdf) or what about past exams?


## Motivation / background

Recently there have been two important observations with AI;

- Deep learning <u>works</u> when done at scale. The bigger the better.
- The ability of strings to represent many different kinds of information allows great flexibility which tasks a LM is trained to do.

This trend of BIGGER deep learning can be seen by;

- More parameters
  - [Gopher](https://arxiv.org/abs/2112.11446). 280B parameters
  - [PaLM](https://arxiv.org/abs/2204.02311). 540B parameters
  - [Switch Transformers](https://arxiv.org/abs/2101.03961). 1T parameters
- More data
  - [MassiveText](https://arxiv.org/abs/2112.11446). 2 trillion tokens
  - [LTIP](https://arxiv.org/abs/2111.02114). 400 million image-caption pairs
- More tasks
  - A generalist agent [GATO](https://www.deepmind.com/publications/a-generalist-agent). Trained on;
    - Simulated control tasks (596 tasks) ([DM Control](https://github.com/deepmind/dm_control), [DM lab](https://www.deepmind.com/open-source/deepmind-lab), [Procgen](https://openai.com/blog/procgen-benchmark/), [Atari ALE](https://github.com/mgbellemare/Arcade-Learning-Environment),  [playroom](https://arxiv.org/abs/1707.03300), ... and more)
    - vision and language (>204 tasks) ([MassiveText](https://arxiv.org/abs/2112.11446), [MultiModal MassiveWeb](https://arxiv.org/abs/2204.14198), [LTIP](https://arxiv.org/abs/2111.02114), [OKVQA](https://okvqa.allenai.org/), ... and more)


Language models

A 'model' of language. Given context predict a distribution over the likely next token.

- "The cat in the _" __->__ "hat" (probably)
- "Monday, Tuesday, Wednesday, _" __->__ Thursday (probably)
- "The most populous city in India is _" __->__ Mumbai (probably)


## Aside: Few-shot meta learning vs fine-tuning

Fine-tune

_What is human life expectancy in the United States?" -> "Human life expectancy in the United States is 78 years._

(we actually train the LM on examples: question -> answer)

Zero-shot

_I am a highly intelligent question answering bot. Ask me questions, I will answer._

_Q: What is human life expectancy in the United States?_

## Aside: Few-shot meta learning vs fine-tuning

Few shot

_I am a highly intelligent question answering bot. Ask me questions, I will answer._

_Q: Who was president of the United States in 1955?_\
_A: Dwight D. Eisenhower was president of the United States in 1955._

_Q: Where were the 1992 Olympics held?_\
_A: The 1992 Olympics were held in Barcelona, Spain._

_Q: What is human life expectancy in the United States?_

(example from openai's api)

## Flexible prompt design

_Language models are all you need_ (for natural language [@Namazifar2020])

As far as we know, it is possible to frame any many tasks as a prompt to a LM.
narrative understanding, textual entailment, entity resolution, question answering, POS tagging, grammatic parsing...

- textual entailment: "text: If you help the needy, God will reward you. hypothesis: Giving money to a poor man has good consequences." -> "positive"  (text entails hypothesis)
- POS tagging: "text: Bob made a book collector happy." -> "subject verb object(article adjective noun) verb-modifier"

## Aside: more fun tasks LMs can do

_Big-Bench_ [@Srivastava2022]

Analyses a LM's ability to do 204 different tasks including.

- auto_debugging
  - "'\\nfor i in range(10):\\n\\ti' What is the value of i the third time line 2 is executed?" __->__ "2"
- color
  - "What is the color most closely matching this RGB representation: rgb(128, 2, 198)?" __->__ "purple"
- chess_state_tracking_legal_moves
  - "e2e4 g7g6 d2d4 f8g7 c1e3 g8f6 f2f3 d7d6 d1" __->__ "c1, d2, d3, e2"

others
- ascii mnist, solve riddles, play sudoku, translate hindi proverbs, idendify math thorems, vitamin C fact verification, etc...

