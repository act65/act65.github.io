---
title: "Note on ??"
layout: post
permalink: /opt-init-const/
---

Consider a constrained optimisation problem.
We want to minimise an objective, while also staying close to a specific point $y$ (the constraint).

Is there a rigorous connection between;

1. an optimisation with a penalty term (the typical approach)
2. a finite-step optimisation started from a specific initialisation

$$
x^* = \arg\min_x f(x) + \lambda D(x, y) \\
D(x, y) = \frac{1}{2} \| x - y \|^2 \\
x_0 \sim \mathcal{N}(0, \sigma^2) \\
$$

***

We have a fixed budget of $T$ steps to optimise a function $f(x)$, and we start from a specific initialisation $x_0$.

$$
x^* = \arg\min_x f(x) \\
x_0 = y
$$


***

What's the point of this? / Questions

- 2. will vary dramatically depending on the optimiser used, the learning rate, 
- 2. would allow easier worst case distance bounds? 
- relatonship to trust region methods?