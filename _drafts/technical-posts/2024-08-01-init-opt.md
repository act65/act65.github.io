---
title: Penalty term optimisation vs. finite-step optimisation
subtitle: The connection between steps and penalties
layout: post
permalink: /opt-init-const/
categories:
    - research
---

<!-- https://colab.research.google.com/drive/1lNCdi9PYQKBwwV7jIzm7Cw_YriNDi6SB#scrollTo=yF6zwcI2DX3i -->

Consider a constrained optimisation problem.
We want to minimise an objective, while also staying close to a specific point $y$ (the constraint).

Is there a rigorous connection between;

1. an optimisation with a penalty term (the typical approach)
2. a finite-step optimisation started from a specific initialisation

## Optimisation with a penalty term

$$
x^* = \arg\min_x f(x) + \lambda D(x, y) \\
D(x, y) = \frac{1}{2} \| x - y \|^2 \\
$$

## Finite-step optimisation

We have a fixed budget of $T$ steps to optimise a function $f(x)$, and we start from a specific initialisation $x_0$.

$$
x^* = \arg\min_x f(x) \\
x_0 = y
$$


***


**Theorem:**
For a smooth, convex function f(x), under gradient descent optimization with learning rate η, there exists a relationship between:
- A λ-penalized optimization with penalty term D(x,y) = ½||x-y||²
- A T-step optimization starting from y

such that their solutions are equivalent up to O(η²).

**Proof:**

1) First, let's establish some conditions:
- Let f(x) be L-smooth: ||∇f(x) - ∇f(y)|| ≤ L||x-y||
- Let f(x) be μ-strongly convex: f(y) ≥ f(x) + ∇f(x)ᵀ(y-x) + (μ/2)||y-x||²

2) For the penalized approach:

$$
\begin{align*}
\nabla L(x) &= \nabla f(x) + \lambda (x-y) \\
\nabla L(x*) &= 0 \tag{at optimality} \\
\nabla f(x*) + \lambda (x*-y) &= 0 \\
\end{align*}
$$

3) For the T-step approach with gradient descent:

$$
\begin{align*}
x_{t+1} = x_t - \eta\nabla f(x_t)
x_0 = y
\end{align*}
$$

4) Key insight: After T steps, the maximum distance from y is bounded:

$$
||x_T - y|| ≤ \eta\sum_{t=0}^{T-1} ||\nabla f(x_t)||
$$

5) Using smoothness and convexity:

$$
||∇f(x_t)|| ≤ ||∇f(y)|| + L||x_t - y||
$$

6) This gives us a recursive bound:

$$
||x_T - y|| ≤ ηT||∇f(y)|| + ηTL·max_{t<T}||x_t - y||
$$

7) For small enough η, this converges to:

$$
||x_T - y|| ≤ ηT||∇f(y)||/(1-ηTL)
$$

8) Comparing with the penalized approach:
From step 2: ||x* - y|| = (1/λ)||∇f(x*)||

9) For these to be equivalent:

$$
1/λ ≈ ηT
$$

Therefore, we can establish that choosing λ = 1/(ηT) makes the penalized approach approximately equivalent to the T-step approach, up to O(η²) terms.

**Corollary:**
The solutions will be closer when:
- η is small
- f(x) is well-conditioned (L/μ is small)
- T is moderate (not too large or small)