---
layout: post
title: Score functions, denoising and diffusion
permalink: /scoredd/
---

<!-- Goal: learn p(x) -->

Our goal is to approximate a data distribution $p(x)$ with a model $p_{\theta}(x)$ using a set of samples, $D_n = \{ x_i: x_i\sim p(x), i \in Z_n \}$.

<!-- Why do we care about p(x)? -->

With this model of the data distribution we can generate our own samples $x\sim p_{\theta}(x)$, and we can evaluate $p_{\theta}(x)=p$. This ability to sample and evaluate facilitate many tasks in machine learning; generative modelling, density estimation, and anomaly detection.

***

<!-- Max likelihood -->

We start formulating the problem as finding a model distribution that minimises the KL divergence to the data distribution.

$$
D_{KL}(p, p_{\theta}) = \mathbb E_{x \sim p(x)} [ \log p(x) - \log p_{\theta}(x) ] \\
$$

This formulation results in maximising the likelihood ($p(x \mid \theta)$) of the data under the model.
The data distribution term ($\log p(x)$) doesnt depend on $\theta$ and can be ignored when optimising $\theta$.

$$
\theta^{* } = \text{argmax}_{\theta} \mathbb E_{x \sim p(x)} [ \log p_{\theta}(x) ] \\
$$

Solving this optimisation problem is hard.
There are many different ways we can construct our model, $p_{\theta}(x)$. However, they require that:

$$
\int p_{\theta}(x) dx = 1
$$

This constraint is, in general, intractable. However, if we are clever, we can avoid this constraint. For example;

- _normalising flows_ construct invertible maps (that preserve density) between distributions. Thus, as long as the initial distribution is normalised, the final distribution will also be normalised.
- _variational autoencoders_ attempt to find a latent space where the data distribution is a simple (ie gaussian) distribution.
- _energy based models_ learn an energy function which corresponds to unnormalised probabilities.

<!-- Other approaches -->
In this blog, we consider yet another approach.

## Model the score indead of the density
<!-- BUT why learn the score? does it allow us to do the same things as p(x)? Yes. Allows sampling and evaluating p(x). -->
An alternative approach to modelling the density is to approximate the score function, $s(x, \theta) \approx \nabla_x \log p(x)$. This allows us to avoid the normalisation constraint.

Instead of the KL divergence (above) which led us to the maximum likelihood solution, we start with the Fisher divergence 
<!--  lyu_interpretation_2009. -->

$$
\begin{align*}
D_F(p, p_\psi) &= \mathbb E_{x \sim p(x)} \parallel \frac{\nabla_x p(x)}{p(x)} - \frac{\nabla p_{\psi}(x)}{p_{\psi} (x)}\parallel^2 \\
\end{align*}
$$

This leads to the score matching objective function 
<!-- hyvarinen_estimation_2011. -->


$$
\begin{align*}
\mathcal L(\theta) &= \mathop{\mathbb E}_{x\sim p(x)} \parallel s(x, \theta) - \nabla_{x} \log p(x) \parallel^2 \\
\end{align*}
$$

However, this function contains $\nabla_x \log p(x)$, the true score function, which we do not have access to. 
Our setting only grants samples from the density. 

Fortunately, there are two known solutions to this problem; score matching 
<!-- hyvarinen_estimation_2011  -->
denoising score matching 
<!-- vincent_connection_2011 -->


***

Quick aside, what is the score function?

<!-- background on score functions -->
In the case where the density is a gaussian distribution, the score function is:

$$
\begin{align*}
p(x) &= \mathcal N(x \mid \mu, \sigma^2I) \\
\nabla_{x} \log p(x) &= \nabla_{x} \left( \log \frac{1}{\sqrt{2\pi \sigma^2}} \exp \left( - \frac{1}{2\sigma^2} \parallel x - \mu \parallel^2 \right) \right)\\
&= \nabla_{x} \left( - \frac{1}{2\sigma^2} \parallel x - \mu \parallel^2 \right) \\
&= \frac{1}{\sigma^2} (\mu - x) \\
\end{align*}
$$

![The score function plotted for a mixture of 3 gaussians. The vector field represents the score function, the heat map represent the density.]({{ site.baseurl }}/assets/scoredd/gaussian-score.png)

### Score matching

<!-- hyvarinen_estimation_2011  -->
show that we can write the gradient of this function in a way that doesn't contain the score function. Thus allowing us to calculate the score function (through gradient descent, or other optimisation methods) without having access to it.

The derivation is as follows;

$$
\begin{align*}
\nabla_\theta \mathcal L(\theta) &= \nabla_\theta \mathop{\mathbb E}_{x\sim p(x)} \Big[ \parallel s(x, \theta) \parallel^2 - 2 \langle s(x, \theta), \nabla_x \log p(x)\rangle + \parallel \nabla_x \log p(x)\parallel^2\Big]\\
&= \nabla_\theta \mathop{\mathbb E}_{x\sim p(x)} \Big[ \parallel s(x, \theta) \parallel^2 - 2 \langle s(x, \theta), \nabla_x \log p(x)\rangle\Big] \\
\end{align*}
$$

We epanded the score matching objective (from above). Now, the last term will drop off since it is not a function of $\theta$, thus we ignore it. Now we can use the log derivative trick (1.) and a partial integration trick (2.) to rewrite the second term.

$$
\begin{align*}
\int p(x)\langle s(x, \theta), \nabla_x \log p(x, \theta)\rangle dx &= \int p(x)\sum_i s(x, \theta)_i \nabla_{x_i} \log p(x) dx\\
&= \int p(x)\sum_i s(x, \theta)_i \frac{\nabla_{x_i} p(x)}{p(x)} dx \tag{1.}\\
&= \int \sum_i s(x, \theta)_i \nabla_{x_i} p(x) dx \\
&= \int \sum_i \nabla_{x_i} s(x, \theta)  p(x) dx \tag{2.}\\
&= \int p(x) \nabla_x \cdot s(x, \theta) dx \\
\end{align*}
$$


$$
\begin{align*}
\nabla_x \log p(x) &= \frac{\nabla_x p(x)}{p(x)} \tag{1.} \\
\int \nabla_x f(x) \cdot g(x)dx &= \int f(x) \cdot\nabla_x g(x)dx \tag{2.}\\
\end{align*}
$$

Thus we have:

$$
\mathcal L_{SM}(\theta) = \mathop{\mathbb E}_{x\sim p(x)} \Big[ \parallel s(x, \theta) \parallel^2  + 2 \nabla_x \cdot s(x, \theta)\Big]
$$

***

Quick aside, what is the divergence?


$$
\begin{align*}
\Delta f &= \nabla \cdot \nabla f = \nabla^2 f \\
\Delta \log p(x) &= \nabla \cdot \nabla \log p(x) = \nabla^2 \log p(x) \\
&\approx \nabla \cdot s(x, \theta) \\
\end{align*}
$$


And relation to laplacian!?
<!-- insert picture?! low vs lower divergence -->


***

To recap, what is this objective doing?

Where we see data points (sampled from $p(x)$) our model of the score fn should have minimal divergence (ie should be a sink).
But not too big a sink (since we are also minimising the norm of the score fn).

<!-- Relationship between the 2-norm of the score and the gaussian dist? -->

<!-- TODO to derive this score matching objective for other distributions?
While would yield pairings of distribution and norm? -->

<!-- ineresting part is that this recovers scores that point in the right direction with right magnitude for gaussian noise.
what if we use a different norm / metric? this will yield a non added gaussian noise score fn.!? -->

***

#### Score matching for ??? distrubutions





<!-- this requires calculating a second derivative. which is expensive!? that's why we prefer DSM? -->

### Denoising score matching

Denoising score matching, 
<!-- Alain2012 vincent_connection_2011  -->
starts by slightly changing the goal. Instead of learning the score function of the data distribution, we learn the score function of a parzen-window density of the empirical data we observe.

$$
\begin{align*}
q(\hat x) &= \int_x p(\hat x \mid x) p(x) dx \\
&= \int_x \mathcal N(\hat x \mid x, \sigma^2I) p(x) dx \\
&\approx \frac{1}{N}\sum_i^N \mathcal N(x \mid x_i, \sigma^2I) \tag{$x_i\sim p(x)$}\\
\end{align*}
$$

Our DSM objective function becomes:

$$
\begin{align*}
\mathcal L(\theta) = \mathop{\mathbb E}_{x\sim p(x)} \mathop{\mathbb E}_{\hat x \sim p(\hat x\mid x)} \parallel s(\hat x, \theta) - \nabla_{\hat x} \log q(\hat x) \parallel^2 \\
\end{align*}
$$

***
<!-- why would we want to learn the score of this noise distribution? -->
Now, how are these two distributions related? What does $\nabla_x \log q(x)$ tell us about $\nabla_x \log p(x)$?

<!-- want to know. do they always point the same direction? -->

<!-- for a finite sample, how wrong are we likely to be? p(x) - p_{\theta}(x) -->

TODO prove...

$$
\lim_{\sigma \to 0} \nabla_{\hat x} \log q_{\sigma}(\hat x \mid x) = \nabla_{x} \log p(x) \\
$$
This is wrong?!?
$$
\lim_{n \to \infty} \nabla_{\hat x} \log q_{\sigma}(\hat x \mid D_n) = \nabla_{x} \log p(x) \\
$$
also. wrong. need sigma to be small!?

Also prove that for large datasets, the difference is small. (assuming some kind of smoothness on $p(x)$)

$$
 p(x) \approx q_{\sigma}(\hat x \mid D_n) \\
q_{\sigma}(\hat x \mid D_n) = \frac{1}{n} \sum_{i=1}^n \mathcal N(\hat x \mid x_i, \sigma) \\
\epsilon = \text{KL}(p(x) \parallel q_{\sigma}(\hat x \mid D_n)) \\
\epsilon < f(n) \\
$$

***

<!-- Want some pics?! 

- $q(\hat x \mid x)$ tells us a lot about $p(x)$.
- score fn arrows pointing towards the x's. -->
Since we have chosen our noise distribution to be gaussian, we can calculate the score function of this distribution.

$$
\begin{align*}
\nabla_{\hat x}\log q(\hat x | x, \sigma) &= -\frac{1}{\sigma^2} (\hat x - x) \\
p(\hat x \mid x, \sigma) &= \mathcal N(\hat x \mid x, \sigma^2I) \\
\nabla_{\hat x}\log \mathcal N(\hat x \mid x, \sigma^2I) &= -\frac{1}{\sigma^2} (\hat x - x)
\end{align*}
$$


Thus our loss becomes:

$$
\begin{align*}
\mathcal L(\theta) &= \mathop{\mathbb E}_{x\sim p(x)} \mathop{\mathbb E}_{\hat x \sim p(\hat x\mid x)} \parallel s(\hat x, \theta) + \frac{1}{\sigma^2} (\hat x - x) \parallel^2 \\
\end{align*}
$$

Where the model $s(\hat x, \theta)$ attempts to predict the (normalised) noise added to $\hat x$. Aka, the score function (for additive gaussian noise).

#### Connection to denoisers

Alternatively, we can learn a denoiser via optimising;

$$
\begin{align*}
D^{* } = \text{argmin}_{D} \mathop{\mathbb E}_{x\sim p(x)} \mathop{\mathbb E}_{\hat x \sim p(\hat x\mid x)} \parallel D(\hat x) - x \parallel^2 \\
\end{align*}
$$

Where $D$ attempts to predict the true value of $x$ given a noisy observation $\hat x$.

Then we can recover the score function of the data distribution by;

$$
\begin{align*}
s(\hat x, \theta) &= \frac{D^{* }(\hat x) - \hat x}{\sigma^2} \\
\end{align*}
$$


<!-- Tweedie's formula
$$
\mathbb E[x\mid \hat x] = \hat x + \sigma^2 \nabla_{\hat x} \log p(\hat x)
$$

[@meng_estimating_nodate]  -->



<!-- Given the structure of this solution. How about parameterising $f(\hat x, \theta) = \hat x + \sigma^2 \nabla_{\hat x} \log f(\hat x, \theta)$. Well some clever people already thought of that. -->

#### Experiments

Score matching vs denoising score matching. Which is better?

<!-- how about in higher dimensions!? divergence would be more reliable than denoising? -->

<!-- try both losses. which learns faster? -->
<!-- Would rather prove which has lower variance when estimated from a finite sample. Or its convergence speed -->

***

Why DSM?

> One way to find an estimate θ for the energy E(x; θ) would be to simply
apply the basic definition of score matching. However, as discussed above, such an approach is often
considered to be computationally prohibitive due to the appearance of second-order derivatives in
the objective function (Kingma & LeCun, 2010) and one must resort to approximations (Martens
et al., 2012a). Our key insight here is to build on the Bayesian interpretation of the score function of
Raphan & Simoncelli and the Parzen score matching of Vincent. This leads to a method which scales
well to high dimensions, as well as to large datasets

https://arxiv.org/pdf/1805.08306.pdf


## Connection between max likelihood and score matching

Instead of the KL divergence, define the fisher divergence.

$$
D_F(p, q) = \mathbb E_{x \sim p(x)} \mid \frac{\nabla_x p(x)}{p(x)} -  \frac{\nabla_x p_{\theta}(x)}{p_{\theta}(x)} \mid^2 \\
$$


$$
D_{KL}(p, q) = \mathbb E_{x \sim p(x)} \log \frac{p(x)}{p_{\theta}(x)} \\
D_{F}(p, q) = \mathbb E_{x \sim p(x)} \mid \nabla_x \log \frac{p(x)}{p_{\theta}(x)} \mid^2 \\
$$

$$
y(t) = x + \sqrt{t} w \\
\frac{d}{dt} D_{KL}(p(y(t), t), p_{\theta}(y(t), t)) = - \frac{1}{2} D_F(p(y(t), t), p_{\theta}(y(t), t)) \\
$$

<!-- [@lyu_interpretation_2009] -->

## Learning the score fn

So far we have covered the what and how of score matching. But we haven't covered the why. 

Why does it work? Is it fundamentally easier to learn the score than the density?

<!-- [@pabbaraju_provable_2023] -->

## Diffusion

> What does all this have to do with diffusion?

<!-- quick reminder what diffusion is -->
In diffusion we construct a forward process to be; $p(x_{t+1} \mid x_t) = \mathcal N(x_{t+1} \mid x_t, \sigma(t)^2I)$.
<!-- [@song_score-based_2021].  -->
We are interested in learning the score of this forward process, so we can reverse the process $p(x_{t-1} \mid x_t)$. Since SDEs are reversible 
<!-- [@song_score-based_2021]. ... -->

Now, imagine that we stack many denoising score matching models on top of each other. Each model is trained to denoise the output of the previous model.

![From left to right we have the addition of increasing amounts of Gaussian noise. Learning to invert the added noise between two neighbors can be solved via denoising score matching.]({{ site.baseurl }}/assets/scoredd/denoising-diffusion.png)

Something is wrong in my interpretation / calculation of this image?
The score fn is not right. Want the transition score fn. p(x | x'). But have plotted p(x).

<!-- NOTE IDEA there is structure here we could exploit / enforce?
the nn, as a fn of t, should become smoother. a good prior? -->




<!-- NOTE So for diffusion we learn N closely related distributions? (one for each time step?) -->


<!-- However, most diffusion models don't do DSM. They are trained to predict $x(t=0)$.
So are they actually learning a score fn? Can we follow this prediction with guarantees? (is it a valid process to generate samples?) -->

Using this framework of stacked denoisers, we can generate new samples. By taking a sample from a gaussian distribution and passing it through each denoiser in turn. But how can we sample using a score function? Are the samples produced by stacking denoisers valid? What distribution do they come from? 
See [Diffusion, transport and flows](www.link.com) for answers.  

## Bibliography
