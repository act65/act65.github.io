---
layout: post
title: The Price of Precision
subtitle: Introducing the Progressive-Fidelity Bandit Problem
---

In the classic Multi-Armed Bandit (MAB) problem, an agent chooses between several slot machines (or "arms") to maximize its reward. It's a powerful metaphor for decision-making under uncertainty. But what if pulling an arm isn't a single action, but a choice with its own set of costs and information quality? What if, before you pull the real arm, you could pay for a less accurate "test" pull?

This post introduces the **Progressive-Fidelity Bandit**, a variation of the MAB problem where we must balance not just exploration and exploitation, but also the cost of information itself.

### The Motivation: When Information Isn't Free

In many real-world scenarios, we have access to a hierarchy of information sources, each with its own cost and accuracy.

1.  **Drug Discovery:** This is the canonical example. A pharmaceutical company can use cheap, fast, but inaccurate computer simulations to predict a drug candidate's efficacy. More accurate quantum mechanics-based simulations are more expensive. Animal testing is even more so. Finally, a full Randomized Controlled Trial (RCT) on humans is incredibly expensive but provides the most accurate data. The "arm" is the drug candidate, and the "fidelity level" is the evaluation method.

2.  **A/B Testing in Tech:** A company wants to test a new feature on its website (the "arm"). It could start with an internal simulation of user behavior (very cheap, low fidelity). It could then run a small-scale "canary" test on 1% of its users (more expensive, more accurate). Finally, it could roll it out to 50% of users in a full A/B test (most expensive, most accurate).

3.  **Minimum Viable Product (MVP) Development:** Consider a startup building a new software product. They have an idea for a core feature ("the arm"). Instead of building the full, polished version, they can test the waters with progressively higher-fidelity versions. A low-fidelity probe could be a simple landing page describing the feature to gauge interest. A medium-fidelity probe might be an interactive prototype. The highest-fidelity action is developing and launching the actual MVP. Each step costs more in time and resources but provides a much clearer signal about the feature's true value.

4.  **Resource Exploration:** An energy company is searching for oil. The "arms" are different geographical locations. The company can use cheap satellite imagery to get a rough idea of the geology. This is a low-cost, low-fidelity probe. Based on those results, it can conduct more expensive and detailed seismic surveys. Finally, the most expensive and accurate action is to drill an exploratory well.

In all these cases, the core decision is not just *which* arm to investigate, but *how much* to pay for the quality of that investigation.

### The Abstract Problem: Bandits with a Price Tag

Let's formalize this. We have two fundamental sets:

*   A set of **K arms**, indexed by `k`. Each arm has a true, unknown reward distribution `D_k`. When we pull arm `k`, we get a reward `r_k ~ D_k`.
*   A set of **M fidelity levels**, indexed by `m`. We can think of these as probes or evaluation methods, `P_0, P_1, ..., P_{M-1}`.

The core of the problem lies in how these two sets interact. We introduce a set of **transformation functions**, `f_m`, where each function `f_m` takes a true distribution `D_k` and adds some noise or bias to produce an observable, lower-fidelity distribution:

`T_{k,m} = f_m(D_k)`

Here, `T_{k,m}` is the reward distribution for probing arm `k` with fidelity level `m`.

-   **Accuracy increases with `m`**. A higher fidelity level `m` should yield a distribution `T_{k,m}` that is "closer" to the true distribution `D_k`. Using the Earth Mover's Distance (EMD) as a measure of the difference between distributions, we have: `EMD(D_k, T_{k,0}) > EMD(D_k, T_{k,1}) > ...`.
-   **Cost increases with `m`**. Each fidelity level `m` has an associated cost, `c(m)`. We expect costs to increase with accuracy, so `c(0) < c(1) < ... < c(M-1)`.

#### The Importance of Cost Scaling

The nature of the cost function `c(m)` is critical. Consider two scenarios:

1.  **Logarithmic or Polynomial Cost** (`c(m) = α * log(m)` or `c(m) = α * m^β`): If the price of higher fidelity grows slowly, an agent might be quick to abandon the cheapest probes. The trade-off is still present, but less stark.
2.  **Exponential Cost** (`c(m) = α * exp(βm)`): This often mirrors reality. The cost of an RCT in drug discovery can be orders of magnitude greater than a computer simulation. In this regime, the agent *must* be frugal. It cannot afford to run high-fidelity probes on unpromising arms. This steep cost curve makes the problem strategically deep, forcing the agent to use cheap probes to aggressively prune the search space.

**What's the Goal?**

How do we define success? The most flexible approach is to incorporate cost directly into the regret calculation.

`Regret(T) = (T * μ*) - (Σ rewards_t - Σ costs_t)`

Here, `μ*` is the mean of the best arm, and the summation is over all time steps `t=1...T`. This **Cost-in-Regret** model forces the agent to learn the trade-off between gaining information and paying for it at every single step.

### A Concrete Example: The Gaussian Case

Let's make this more concrete. Assume the true reward for each arm `k` comes from a Gaussian distribution with an unknown mean `μ_k` and a known variance `σ_k^2`.
`D_k = N(μ_k, σ_k^2)`

Our goal is to find the arm with the highest `μ_k`.

We can model the transformation `f_m` as a process that adds Gaussian noise. The reward from probing arm `k` at fidelity level `m` is a sample from:
`T_{k,m} = N(μ_k, σ_k^2 + v_m)`

Here, `v_m` is the variance added by the probe `m`. For the probes to become more accurate, the added noise must decrease as `m` increases:
`v_0 > v_1 > ... > v_{M-1} ≥ 0`

A probe `m=0` is very noisy (`v_0` is large), while a higher-level probe is much less noisy. Let's pair this with an exponential cost function `c(m) = α * exp(βm)`.

The agent's decision at each step is now two-fold:
1.  **Which arm `k` should I investigate?** (The classic exploration/exploitation choice).
2.  **Which fidelity level `m` should I use for this investigation?** (The new cost/accuracy choice).

### The Path Forward: Heuristics and Information Theory

This problem can be viewed as a `K x M` MAB problem, where each action is the pair `(k, m)`. However, this ignores the crucial structure: learning from `(k, m_1)` also tells you something about `(k, m_2)`. Efficient algorithms must exploit this.

The strategy an intelligent agent might discover mirrors a very human heuristic: **start broad and cheap, then get narrow and expensive.** An optimal agent would likely use low-cost, low-fidelity probes to quickly eliminate obviously bad arms. Then, for the remaining promising candidates, it would progressively pay for higher-fidelity probes to confidently identify the winner. This formalizes the general problem-solving technique of breaking a complex question into smaller, more manageable parts.

There is also a deep connection to **Information Theory**. At its core, this is a problem of optimal information acquisition. Each probe `(k, m)` can be seen as buying a piece of information that reduces our uncertainty (or entropy) about the true value of arm `k`. An advanced agent could be designed to explicitly maximize the **information gain per unit cost**. It would, at each step, choose the probe `(k, m)` that offers the best "bang for your buck" in terms of reducing uncertainty about the best arm. This perspective shifts the focus from simply balancing exploration and exploitation to managing an economy of information.

<!-- 

TODO work in;

    A more powerful approach, however, may come from Information-Directed Sampling (IDS), a framework developed by Russo and Van Roy (2017) [^1]. IDS is designed for problems with complex information structures like this one. It works by selecting actions that minimize the ratio of squared regret to information gain, where gain is measured using mutual information.

    This framework provides a formal basis for the very heuristics we've discussed: using cheap, low-fidelity probes to efficiently explore the problem space. An IDS agent would naturally learn to manage an 'economy of information,' selecting the probe that offers the most uncertainty reduction for the lowest cost, making it a perfect candidate for solving the Progressive-Fidelity Bandit problem."


ref [1^]: https://arxiv.org/pdf/1403.5556
 -->


The path forward involves adapting classic algorithms like UCB or Thompson Sampling to navigate this new, richer, two-dimensional action space, creating agents that are not just explorers, but also savvy information economists.

<!-- 
hrmm. there's a problem with our setting?
we use the rewards as both; the observed quantity we are interested in learning about, the thing we are trying to optimise.

the problem comes when we pick a f that (say) adds a bias to the true reward.
what value do we put in the regret? the observed / sampled reward (with added bias)) or the true reward?
-->

<!-- 
for f just adding variance to D_k.
and K=1 (only 1 guassian arm. we are just picking the fidelity.)
convergence of mean is error(k) = stddev_k/sqrt(n) (i think).
n = O(stddev^2/error^2).
so with enough samples, we will get an accurate estimate.
the optimal choice of fidelity will depend on the costs.
if budget / costs(k) = ?? then...
ratio of costs to ratio of reductions in error!?
help me finish this argument.
  -->

<!-- 
TODO find a connection to The Hidden Cost of Waiting for Accurate Predictions https://openreview.net/forum?id=A3YUPeJTNR
the reward fns can change over time? or become unavailable?
every 'opportunity' (possible arm / reward) has some survival probability?
 -->


<!-- 
what insights can we expect from this analysis?

in some cases it's optimal to follow a kind of curriculum? which cases!?
by curriculum i mean: start with low fidelity on many arms. only progress to higher fidelity after finished low fidelity? use the info from the earlier exploration to decide to skip higher fidelity tests for some arms.

in the earlier single gaussian arm setting, i think it's clear there will be one fidelity that is optimal. shouldn't bother with the rest.

???
-->

<!-- 
what's a setting that matches this exactally?
f included?!
 -->