---
title: Geometric Bayes via Wasserstein
subtitle: Geometric vs. Informational Beliefs
layout: post
---

### A Tale of Two Constraints: Geometric vs. Informational Beliefs

In the previous post, we saw that Bayesian updating is the optimal solution to a specific problem: minimizing the KL divergence (informational "surprise") from a prior, subject to a constraint on the average log-likelihood of the data.

But we've also established that when our objective is to minimize the geometric "work" of an update (the Wasserstein distance), the log-likelihood constraint feels like it's from a different philosophical family. It mixes a geometric objective with an informational constraint.

This begs the question: What if we went all-in on geometry? Can we construct a data constraint that is *also* based on the Earth Mover's Distance? What kind of update rule does this "purely geometric" framework produce?

### Forging a Geometric Constraint from Data

The first challenge is to represent the information from our data `D` in a geometric way. The Wasserstein distance measures the distance between two probability distributions, but our data `D` is just a set of observations.

The most natural way to bridge this gap is to have the data define its own "ideal" target distribution.

1.  **The Data's Best Guess: The MLE**
    First, we ask: which single parameter value `θ` would make the data we observed most probable? This is the classic **Maximum Likelihood Estimate (MLE)**.

    $$
    \theta_{MLE} = \underset{\theta}{\text{argmax}} \, p(D\mid\theta)
    $$

    The MLE represents the single point in our parameter space that the data advocates for most strongly.

2.  **The Data's Ideal Belief: The Dirac Delta**
    Next, we represent this "best guess" as a probability distribution. We use the **Dirac delta distribution**, `δ(θ - θ_MLE)`. This is a special distribution that has zero density everywhere except at the single point `θ_MLE`, where it has an infinitely concentrated spike of mass. The total mass is still 1.

    Let's call this the data's target distribution, `p_D(\theta) = δ(\theta - \theta_{MLE})`. It represents a state of absolute certainty, based *only* on the data, that the true parameter is `θ_MLE`.

Now we have two distributions: our prior `p(θ)` and the data's target `p_D(θ)`. We can use the Wasserstein distance to measure the distance between them.

### The New Optimization Problem: A Purely Geometric Framework

With these tools, we can state our new, purely geometric optimization problem.

**Objective:** Find the posterior distribution `q(\theta)` that minimizes the squared Wasserstein-2 distance from the prior `p(\theta)`. We want to do the minimum amount of "work" to move our beliefs.

$$
\min_{q} W_2^2(q, p)
$$

**Constraint:** The resulting posterior `q(\theta)` must be "close" to the data's target distribution `p_D(\theta)`. We enforce this by setting a budget, `B`, on how far `q` can be from `p_D`.

$$
W_2^2(q, p_D) \le B
$$

The full problem is a constrained optimization:

$$
\underset{q}{\text{argmin}} \, W_2^2(q, p) \quad \text{subject to} \quad W_2^2(q, p_D) \le B
$$

The intuition is beautiful: **Find a new belief system `q` that is as close as possible to your prior `p`, while staying within a "distance budget" `B` of the data's ideal belief `p_D`**.

### The Derivation: The Path of Least Resistance

Solving this problem requires a key result from optimal transport theory. The space of probability distributions, when equipped with the Wasserstein metric, has a geometry. The shortest path between any two distributions `p_0` and `p_1` is called a **Wasserstein geodesic**.

The solution to our constrained optimization problem will lie on the geodesic connecting our prior `p` to the data's target `p_D`.

Let's think about the transport map.
*   The map that transports `p` to `p_D` is the one that moves every point `θ'` in the support of `p` to the single point `θ_MLE`. Let's call this map `T_{p \to p_D}(\theta') = \theta_{MLE}`.
*   A geodesic path between `p` and `p_D` can be parameterized by a time `t \in [0, 1]`. A distribution `q_t` on this path is formed by moving every point `θ'` in the prior a fraction `t` of the way towards its destination.

The transport map `T_t` that generates the distribution `q_t` is a simple linear interpolation:

$$
T_t(\theta') = (1-t) \cdot \theta' + t \cdot T_{p \to p_D}(\theta')
$$

Substituting our target map, we get:

$$
T_t(\theta') = (1-t)\theta' + t \cdot \theta_{MLE}
$$

Our optimal posterior `q` must be one of these `q_t`'s. The specific value of `t` will be chosen to perfectly satisfy the budget constraint `B`. A smaller budget `B` (a stricter data constraint) will require a larger `t`, pushing our beliefs closer to the MLE.

### The Form of the Solution: A Shrinkage Update

The optimal transport map that defines our new belief system is:

$$
T(\theta') = (1-t)\theta' + t \cdot \theta_{MLE}
$$

This is the form of our update. Let's analyze what it means.

To find the new location for a "particle" of belief that started at `θ'`, you don't re-weight it. Instead, you **move it along the straight line towards the Maximum Likelihood Estimate**.

The parameter `t` (determined by our data-closeness budget `B`) controls *how far* along that line you move it.

*   If `t=0`, the map is `T(\theta') = θ'`. There is no update. The posterior equals the prior. This corresponds to an infinitely large budget `B`.
*   If `t=1`, the map is `T(\theta') = \theta_{MLE}`. All belief is transported to the MLE. The posterior becomes the Dirac delta `p_D`. This corresponds to a budget `B=0`.
*   If `0 < t < 1`, the posterior `q` is a version of the prior that has been **shrunken** towards the MLE and shifted.

This type of update is known as a **shrinkage estimator**. It's a convex combination of your prior belief (`θ'`) and the data's evidence (`θ_MLE`).

| | **Informational Constraint (Score Update)** | **Geometric Constraint (Shrinkage Update)** |
| :--- | :--- | :--- |
| **Update Rule** | $T(\theta') = \theta' + \lambda \nabla\log p(D\mid \theta')$ | $T(\theta') = (1-t)\theta' + t \cdot \theta_{MLE}$ |
| **Interpretation** | Move beliefs up the **gradient** of the log-likelihood surface. | Move beliefs along a **straight line** towards the MLE. |
| **Key Parameter** | `λ`: The "step size" of the gradient ascent. | `t`: The "fraction" of the distance to travel towards the MLE. |
| **Nature of Update** | A local, differential update based on the slope of the likelihood. | A global, geometric update based on the location of the likelihood's peak. |


***

### The Ideal Case: When the Gradient IS the Straight Line

Your intuition is perfectly correct for any situation where the log-likelihood function is **quadratic**. This is an incredibly important case, as it includes situations where the likelihood is a Gaussian distribution, which is central to statistics (e.g., linear regression with Gaussian noise).

Let's prove this. A general quadratic log-likelihood centered at `θ_MLE` can be written as:

$$
\log p(D\mid\theta) = -\frac{1}{2} (\theta - \theta_{MLE})^T A (\theta - \theta_{MLE}) + C
$$

where `A` is a positive-definite matrix (which controls the "width" of the peak) and `C` is a constant. The peak of this function is clearly at `θ = θ_MLE`.

Now, let's find the gradient of this function with respect to `θ`:

$$
\nabla_{\theta} \log p(D\mid\theta) = -A (\theta - \theta_{MLE})
$$

This result is profound. It shows that the gradient vector `∇log p(D|θ)` is just the vector pointing from our current location `θ` to the MLE, `(θ_MLE - θ)`, scaled by the matrix `A`.

**The gradient points directly along the straight line to the MLE.**

In this common scenario, the two update rules we derived become fundamentally the same:

1.  **The Score Update (from the informational constraint):**
    `T(\theta') = \theta' + \lambda \nabla\log p(D|\theta') = \theta' + \lambda A(\theta_{MLE} - \theta')`
    This moves `θ'` along the direction `(θ_{MLE} - \theta')`.

2.  **The Shrinkage Update (from the geometric constraint):**
    `T(\theta') = (1-t)\theta' + t \cdot \theta_{MLE} = \theta' + t(\theta_{MLE} - \theta')`
    This also moves `θ'` along the direction `(θ_{MLE} - \theta')`.

The two forms are equivalent; the parameters `λ` and `t` simply play the same role of controlling the step size. This means that when the likelihood surface is well-behaved (like a Gaussian), the "informational" and "geometric" frameworks lead to the exact same update path. You don't need to find the MLE beforehand; you can simply follow the local gradient, and it will guide you along the straight line path to the peak.

### The General Case: The Gradient as a Local Guide

What if the log-likelihood surface is not a simple quadratic? Imagine it's a winding, curved ridge leading up to a mountain peak (`θ_MLE`).

*   **The Shrinkage Update (`θ_MLE - θ'`):** This vector points in a straight line to the summit, even if that line goes through the middle of the mountain. It's the true "as the crow flies" direction. To compute it, you must first know the exact coordinates of the summit.

*   **The Score Update (`∇log p(D|θ')`):** This vector points in the direction of steepest ascent *from your current location*. It points straight up the slope of the ridge. This may not be the same as the direction to the summit, but it's the best possible move you can make based only on local information.



*Image Description: A contour plot of a curved ridge leading to a peak labeled 'θ_MLE'. A point 'θ' is on the ridge. The 'Score Update' vector (gradient) points up the steepest part of the ridge. The 'Shrinkage Update' vector points in a straight line from 'θ' to 'θ_MLE'.*

### Conclusion: Do We Need the MLE?

You have uncovered the crucial difference between a **local** and a **global** update.

1.  **The purely geometric "Shrinkage Update" is global.** It is defined by the endpoint `θ_MLE`. To use it, you must first solve the optimization problem `argmax p(D|θ)` to find that endpoint.

2.  **The informational "Score Update" is local.** It is defined by the gradient at your current position. You do *not* need to know where the peak is. You just follow the local slope, and it will guide you on a path of increasing likelihood.

So, to answer your question directly: **No, you don't need to find `θ_MLE` if you use the gradient.** Using the gradient is a different, locally-guided approach.

The beauty you've stumbled upon is that for many of the most important problems in statistics and machine learning, these two approaches coincide. The local guide (the gradient) is so good that it points directly towards the global destination (the MLE). This deep consistency is a powerful indicator that both frameworks are capturing a fundamental aspect of what it means to learn from data.