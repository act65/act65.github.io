

### A Tale of Two Constraints: Geometric vs. Informational Beliefs

In the previous post, we saw that Bayesian updating is the optimal solution to a specific problem: minimizing the KL divergence (informational "surprise") from a prior, subject to a constraint on the average log-likelihood of the data.

But we've also established that when our objective is to minimize the geometric "work" of an update (the Wasserstein distance), the log-likelihood constraint feels like it's from a different philosophical family. It mixes a geometric objective with an informational constraint.

This begs the question: What if we went all-in on geometry? Can we construct a data constraint that is *also* based on the Earth Mover's Distance? What kind of update rule does this "purely geometric" framework produce?

### Forging a Geometric Constraint from Data

The first challenge is to represent the information from our data `D` in a geometric way. The Wasserstein distance measures the distance between two probability distributions, but our data `D` is just a set of observations.

The most natural way to bridge this gap is to have the data define its own "ideal" target distribution.

1.  **The Data's Best Guess: The MLE**
    First, we ask: which single parameter value `θ` would make the data we observed most probable? This is the classic **Maximum Likelihood Estimate (MLE)**.

    $$
    \theta_{MLE} = \underset{\theta}{\text{argmax}} \, p(D\mid\theta)
    $$

    The MLE represents the single point in our parameter space that the data advocates for most strongly.

2.  **The Data's Ideal Belief: The Dirac Delta**
    Next, we represent this "best guess" as a probability distribution. We use the **Dirac delta distribution**, `δ(θ - θ_MLE)`. This is a special distribution that has zero density everywhere except at the single point `θ_MLE`, where it has an infinitely concentrated spike of mass. The total mass is still 1.

    Let's call this the data's target distribution, `p_D(\theta) = δ(\theta - \theta_{MLE})`. It represents a state of absolute certainty, based *only* on the data, that the true parameter is `θ_MLE`.

Now we have two distributions: our prior `p(θ)` and the data's target `p_D(θ)`. We can use the Wasserstein distance to measure the distance between them.

### The New Optimization Problem: A Purely Geometric Framework

With these tools, we can state our new, purely geometric optimization problem.

**Objective:** Find the posterior distribution `q(\theta)` that minimizes the squared Wasserstein-2 distance from the prior `p(\theta)`. We want to do the minimum amount of "work" to move our beliefs.

$$
\min_{q} W_2^2(q, p)
$$

**Constraint:** The resulting posterior `q(\theta)` must be "close" to the data's target distribution `p_D(\theta)`. We enforce this by setting a budget, `B`, on how far `q` can be from `p_D`.

$$
W_2^2(q, p_D) \le B
$$

The full problem is a constrained optimization:

$$
\underset{q}{\text{argmin}} \, W_2^2(q, p) \quad \text{subject to} \quad W_2^2(q, p_D) \le B
$$

The intuition is beautiful: **Find a new belief system `q` that is as close as possible to your prior `p`, while staying within a "distance budget" `B` of the data's ideal belief `p_D`**.

### The Derivation: The Path of Least Resistance

Solving this problem requires a key result from optimal transport theory. The space of probability distributions, when equipped with the Wasserstein metric, has a geometry. The shortest path between any two distributions `p_0` and `p_1` is called a **Wasserstein geodesic**.

The solution to our constrained optimization problem will lie on the geodesic connecting our prior `p` to the data's target `p_D`.

Let's think about the transport map.
*   The map that transports `p` to `p_D` is the one that moves every point `θ'` in the support of `p` to the single point `θ_MLE`. Let's call this map `T_{p \to p_D}(\theta') = \theta_{MLE}`.
*   A geodesic path between `p` and `p_D` can be parameterized by a time `t \in [0, 1]`. A distribution `q_t` on this path is formed by moving every point `θ'` in the prior a fraction `t` of the way towards its destination.

The transport map `T_t` that generates the distribution `q_t` is a simple linear interpolation:

$$
T_t(\theta') = (1-t) \cdot \theta' + t \cdot T_{p \to p_D}(\theta')
$$

Substituting our target map, we get:

$$
T_t(\theta') = (1-t)\theta' + t \cdot \theta_{MLE}
$$

Our optimal posterior `q` must be one of these `q_t`'s. The specific value of `t` will be chosen to perfectly satisfy the budget constraint `B`. A smaller budget `B` (a stricter data constraint) will require a larger `t`, pushing our beliefs closer to the MLE.

### The Form of the Solution: A Shrinkage Update

The optimal transport map that defines our new belief system is:

$$
T(\theta') = (1-t)\theta' + t \cdot \theta_{MLE}
$$

This is the form of our update. Let's analyze what it means.

To find the new location for a "particle" of belief that started at `θ'`, you don't re-weight it. Instead, you **move it along the straight line towards the Maximum Likelihood Estimate**.

The parameter `t` (determined by our data-closeness budget `B`) controls *how far* along that line you move it.

*   If `t=0`, the map is `T(\theta') = θ'`. There is no update. The posterior equals the prior. This corresponds to an infinitely large budget `B`.
*   If `t=1`, the map is `T(\theta') = \theta_{MLE}`. All belief is transported to the MLE. The posterior becomes the Dirac delta `p_D`. This corresponds to a budget `B=0`.
*   If `0 < t < 1`, the posterior `q` is a version of the prior that has been **shrunken** towards the MLE and shifted.

This type of update is known as a **shrinkage estimator**. It's a convex combination of your prior belief (`θ'`) and the data's evidence (`θ_MLE`).

| | **Informational Constraint (Score Update)** | **Geometric Constraint (Shrinkage Update)** |
| :--- | :--- | :--- |
| **Update Rule** | `T(\theta') = \theta' + \lambda \nabla\log p(D|\theta')` | `T(\theta') = (1-t)\theta' + t \cdot \theta_{MLE}` |
| **Interpretation** | Move beliefs up the **gradient** of the log-likelihood surface. | Move beliefs along a **straight line** towards the MLE. |
| **Key Parameter** | `λ`: The "step size" of the gradient ascent. | `t`: The "fraction" of the distance to travel towards the MLE. |
| **Nature of Update** | A local, differential update based on the slope of the likelihood. | A global, geometric update based on the location of the likelihood's peak. |