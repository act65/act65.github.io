---
title: "Automatic differentiation"
subtitle: "Not symbolic, not numerical, but automatic"
layout: post
permalink: /autoint/autograd
categories:
    - "tutorial"
scholar:
  bibliography: "auto-int.bib"
---

## Automatic differentiation

Automatic differentiationis a method for computing the derivative of a function. It is a generalisation of the chain rule and the product rule. It is also known as algorithmic differentiation, computational differentiation, and sometimes just autodiff or AD.

### Why is it useful?

AD allows us to efficiently compute the derivative of a function without having to explicitly write down a closed form expression for the derivative. This is useful because it allows us to compute the derivative of functions that are difficult to differentiate by hand, or functions that are difficult to differentiate symbolically.

AD has many applications in machine learning, where we often need to compute the derivative of a function that is defined by a computer program. For example, we might want to compute the derivative of a loss function with respect to the parameters of a neural network. In this case, the loss function is defined by a computer program, and it is difficult to write down a closed form expression for the derivative of the loss function.

### How does it work?

AD works by building a computational graph of the function that we want to differentiate. The computational graph is a directed acyclic graph (DAG) that represents the function as a sequence of primitive operations. Each node in the graph represents a primitive operation, and the edges represent the flow of data between the operations. The graph is then evaluated using the chain rule to compute the derivative of the function.

### Calculus background

The derivative of a function $f(x)$ is defined as;

$$
\frac{df(x)}{dx} = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}
$$

Using this definition we can derive the chain rule and the product rule. Together these rules allow us to compute the derivative of any function that can be expressed as a composition of primitive operations.

THe chain rule is used to compute the derivative of a function that is defined as a composition of two functions $f(x)$ and $g(x)$.

$$
y = f(g(x)) \\
y = f(u) \\
u = g(x) \\
\frac{dy}{dx} = \frac{y}{u}\frac{u}{x}
$$

The product rule is used to compute the derivative of a function that is defined as the product of two functions $f(x)$ and $g(x)$.

$$
\frac{d f(x) g(x)}{dx} = f(x)\frac{dg(x)}{dx} + g(x)\frac{df(x)}{dx}
$$

### How is it implemented?

(ie how does jax work?)

### How is it different from symbolic differentiation?

Symbolic differentiation is a method for computing the derivative by building a symbolic expression for the derivative.

Computational complexity of symbolic differentiation

- higher order derivatives
- derivatives of fns with many inputs
- 



...


## Automatic differentiation

<!-- what is AD? -->
The key to automatic differentiation is the chain rule.
It allows us to compute the derivative of a function by breaking it down into a sequence of elementary operations.
Calculating their derivatives independently, we can then combine them to get the derivative of the original function.