---
layout: post
title: Sample complexity
subtitle: The Probably approximately correct framework
---

A key question in machine learning is: how much data do we need to train a model? The **Probably Approximately Correct (PAC)** learning framework, developed by Leslie Valiant in 1984, provides a mathematical foundation for answering this question. It helps us understand the relationship between the amount of training data, the complexity of the model, and the model's performance on unseen data.

### The PAC Framework: A Mathematical Guarantee

The PAC framework provides a formal definition of what it means for a learning algorithm to succeed. It states that a concept is PAC-learnable if, for any given error tolerance (ε) and confidence level (δ), there exists a learning algorithm that, after processing a "reasonable" number of training examples, can produce a hypothesis that is "probably" "approximately correct."

Let's break down these terms:

*   **Approximately Correct:** The hypothesis produced by the learning algorithm doesn't have to be perfect. Its error rate on new, unseen data must be less than a small, predefined value ε (epsilon).
*   **Probably:** The guarantee of being approximately correct is not absolute. There's a small probability, δ (delta), that the algorithm will produce a hypothesis with an error greater than ε.
*   **Sample Complexity:** The "reasonable" number of training examples required is known as the sample complexity. This is the key value we want to find.

### A Simple Start: The One-Sided Interval

Let's consider the simplest possible learning problem. Our data points are positive real numbers, and we want to learn a classifier that identifies a specific interval `c = [0, a]`, where `a` is some unknown positive number. Any point within this interval is labeled positive, and any point outside is negative.

Our learning algorithm is given a set of `m` labeled training examples drawn from some unknown probability distribution D. The algorithm's task is to output a hypothesis, `h`, which is another interval of the form `[0, x_max]`, that is a good approximation of the true interval `c`.

The most intuitive algorithm is to simply set `x_max` to be the largest positive example seen in our training data.

***
**Image Description 1:** A number line from 0 to a value greater than 'a'. There is a blue bracketed interval labeled `c = [0, a]` representing the true concept. Inside it, there is a slightly shorter red bracketed interval labeled `h = [0, x_max]` representing our hypothesis. The small region between `x_max` and `a` is shaded and labeled "Error Region."
***

### Deriving the Sample Complexity

How many examples, `m`, do we need to be confident that our learned interval `h` is a good approximation of the true interval `c`?

The error of our hypothesis `h` is the probability that it misclassifies a new, unseen data point. Since our hypothesis `h` is the tightest possible fit to the positive training data, it can never misclassify a negative point as positive. An error can only occur if a new *positive* point from the true interval `c` falls *outside* our learned interval `h`.

This "error region" is the single interval `(x_max, a]`. The total error, which we'll call `err(h)`, is the probability mass of this region according to the data distribution D.

We want to ensure this error is small, specifically `err(h) <= ε`. A "bad" outcome for us is if our learned hypothesis `h` is not approximately correct, meaning `err(h) > ε`. This happens if our training data is unrepresentative, causing `x_max` to be too far from the true `a`.

Let's formalize this to find our sample complexity bound:

1.  Let's define a "bad" region of data. This is the strip of probability mass of size ε located just to the left of `a`. If our learned `x_max` falls to the left of this entire strip, our error will be greater than ε.
2.  Our hypothesis `h` will have an error greater than ε only if *all* `m` of our training samples happen to miss this "bad" strip of probability.
3.  The probability of a single random sample *not* falling into this strip is `1 - ε`.
4.  Therefore, the probability that *all* `m` independent samples miss this strip is `(1 - ε)^m`.
5.  This is our failure probability. We want this probability to be less than our confidence parameter, δ. This gives us the inequality: `(1 - ε)^m <= δ`.

***
**Image Description 2:** The same number line showing the true interval `c = [0, a]`. A region just to the left of `a` is shaded and labeled "Bad Strip of probability ε". Several dots representing training samples (`x_1`, `x_2`, ...) are shown, all falling to the left of this bad strip, illustrating the failure case.
***

6.  Now, we just need to solve this inequality for `m`. We can use the very useful mathematical inequality `1 - x <= e^(-x)`, which is a good approximation for small `x`. Applying this, we get: `e^(-εm) <= δ`.
7.  Solving for `m`:
    *   Take the natural logarithm of both sides: `ln(e^(-εm)) <= ln(δ)`
    *   `-εm <= ln(δ)`
    *   Multiply by -1 and flip the inequality sign: `εm >= -ln(δ)`
    *   Rewrite using logarithm properties: `εm >= ln(1/δ)`
    *   Isolate `m`: `m >= (1/ε) * ln(1/δ)`

This final expression is the **sample complexity** for our simple problem. It tells us the minimum number of samples `m` we need to collect to guarantee that, with a probability of at least `1 - δ`, our learned interval has an error no greater than `ε`.

### Finite vs. Asymptotic Sample Complexity

The result we just derived is an example of a **finite sample complexity** bound. It provides a concrete number of samples required to achieve a certain performance level for any given (finite) `m`, `ε`, and `δ`. For example, if you want 95% confidence (δ=0.05) and an error rate of at most 1% (ε=0.01), you can calculate the number of samples you need.

In contrast, **asymptotic sample complexity** describes the behavior of the learning process in the limit, as the desired error `ε` approaches zero or the number of samples `m` goes to infinity. It's often expressed using "Big O" notation. For our interval example, the asymptotic sample complexity is `O((1/ε)log(1/δ))`.

*   **Finite sample complexity** gives a practical, non-asymptotic bound that can be used to guide data collection in real-world scenarios.
*   **Asymptotic sample complexity** provides a more general understanding of how the required number of samples scales with the error and confidence parameters. It helps in comparing the efficiency of different learning algorithms in the long run.

### References

1.  [University of Pennsylvania CIS 519: Computational Learning Theory](https://www.cis.upenn.edu/~cis519/fall2019/lectures/Lec20_PAC.pdf)
2.  [Heinz, J. (2011). The PAC Learning Framework.](https://www.cs.umd.edu/~inter/papers/heinz_11_pac.pdf)
3.  [PAC Learning - A. Garna, G. Terna](https://users.dimi.uniud.it/~agarna/courses/ml/2017/pac.pdf)
4.  [Valiant, L. G. (1984). A theory of the learnable. Communications of the ACM, 27(11), 1134-1142.](https://doi.org/10.1145/1968.1972)
5.  [Probably approximately correct learning - Wikipedia](https://en.wikipedia.org/wiki/Probably_approximately_correct_learning)