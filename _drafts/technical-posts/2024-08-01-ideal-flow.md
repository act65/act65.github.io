---
title: The behaviour of ideal generative flows
layout: post
permalink: /ideal-si/
scholar:
  bibliography: "ideal-si.bib"
---

Stocahastic interpolants are a recent innovation that frames generative modelling as building a transport map between distributions. {% cite albergo_stochastic_2023 liu_flow_2022 lipman_flow_2023 %}

We are given two distributions, $p(x)$ and $q(x)$ over the same space $X$. Our goal is to find a vector field $v$ that allows us to map from $p(x)$ to $q(x)$.
We can do so by minimising the following objective;

$$
\begin{aligned}
b(z, t) &= \mathbb E\big[\nabla_t I(x, y, t) \big] \\
\mathcal L(\theta) &= \int_0^1 \mathbb E \big[ \parallel v(z, t, \theta) -  b(z, t) \parallel_2^2 \big] dt \\
\end{aligned}
$$

Where $I(x, y, t)$ is the interpolant function, and $v(z, t, \theta)$ is the parameterised vector field.

Here we explore the behaviour of transport maps generated by;

- stochastic interpolants / linear flows
- optimal transport maps
<!-- what about a comparison to NODE or ?? -->

## Stochastic interpolants

Here are a few examples of what stochastic interpolants do.
\footnote{these SI transport maps can be calulated exactly for gaussian distributions. So we can verify that the behaviour we observe is not due to the approximations made by a neural network.}

### Splitting modes

> __Q:__ If I sample from a mode of p(x), must it map to a mode of q(x)? No.

![]({{ site.baseurl }}/assets/ideal-si/gaussian-splitting-2d.png)

> We have two 2D gaussian distributions (in blue and cyan). We use SI to learn a map from p(x) (blue) to q(x) (cyan). The learned mapping 'splits' the modes in p(x) when mapping from p(x) to p(y). ie if we sample from a mode in p(x) (circle or dot) we get 50:50 samples from modes in q(x). Note this mapping approximated using a neural network.

In the more tivial case, if we map from a single gaussian distribution to a multi modal gaussian distribution, then of course the mode of the single gaussian must be split.

### Maximum likelihood sample

> __Q:__ If I take the max likelihood sample from p, must it map to a max likelihood sample from q?

![fig]({{ site.baseurl }}/assets/ideal-si/gaussian-1d-max-like.png)

> Taking the max likelihood x from p(x), we generate a sample using our mapping from p(x) to q(x). We start wih $p(x) \approx 0.8$ and we calculate $p(y\mid x) \approx 0.07$.

So it's possible for a high probability sample from $p(x)$ to map to a low probability sample in $q(x)$!

This observation calls into question the validity / reliability of using flow based approaches to generate solutions to problems with optimal solutions; such as [sudoku](https://arxiv.org/abs/2210.11633), [source separation](https://ieeexplore.ieee.org/document/10095310/), ...

### Mapping the identity

> __Q:__ If I learn to map from $p(x)$ to $q(x)$, then I should learn the identity map? No.

![fig3]({{ site.baseurl }}/assets/ideal-si/gaussian-1d-identity.png)
    
> The learned map from $p(x)$ to $q(x)$ when $p(x) \mathop{=}_d q(x)$.

Mapping from $p(x)$ to $p(x)$ learns a non-linear transform.
However, it is possible to 'rectify' the flow. See [@liu_flow_2022] for more details.

<!-- While it may seem strange to see such an non-uniform mapping. It is the result of The all to all pairings. -->


<!-- ### Topology of modes

This should be preserved?! -->



## Optimal transport maps

WIP

<!-- ## Thoughts

Consider the problem of speech enhancement. The  -->

<!-- None of this matters for the speech - noisy speech setting since the feature spaces will align. And solving an optimal transport problem should give us good results (since p(x) and p(y) share similar feature interpretations. ie X and Y represent the same state space). and p(y) is (approximately) a slightly higher variance version of p(x) (ie convolved with a blurring gaussian). -->



<!-- - how similar do these spaces need to be? -->

## Alternative setting

The 'unsupservised translation' problem hints at an alternative setting. [@grave_unsupervised_nodate] show it's possible to "infer a bilingual lexicon, without supervised data, by aligning word embeddings trained on monolingual data".

So, imagine a setting where we have two 'similar' distributions on different spaces. We want to find a way to 'align' them.

<!-- want a kind of topology preserving map. can be done by enforcing a cost to local changes? -->

$$
T p(x) \to q(y) \\
X \neq Y
$$

Other applicatons could include;

- unsupervised phoneme translation (or accent 'correction')
- unsupervised 

Open questions;

- is it possible to achieve this within the transport framework (with the right cost function)?
- 


## Discussion

More generally, which other properties (modes, max likelihood, ) of a distribution are (not) invariant to the transport map? 

\begin{align}
??
\end{align}

  if we are mapping from text to images, then X and Y are clearly different. But if we are mapping from 

<!-- HOW DOES HIGH DIMENSIONALITY AFFECT THESE OBSERVATIONS -->

## Bibliography

{% bibliography %}