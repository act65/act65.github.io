---
title: tbd
subtitle: tbd
layout: post
categories:
    - economic
---

### **The Proving Ground: An Engineering Approach to Building Aligned Organizations**

In my [this post]({{ site.baseurl }}/alignment), I argued that our society is facing two alignment problems, not one. The first is the distant, speculative risk of a misaligned superintelligence. The second is the present, catastrophic harm being caused by misaligned corporations. The crucial link is this:

> **Our ongoing failure to align corporations with the public good is a live demonstration of our inability to solve alignment problems.**

If we cannot trust a modern corporation—an entity legally bound to prioritize profit above all else—to not harm society, how can we possibly trust it to build a safe and beneficial Artificial General Intelligence? The premise is absurd. The vehicle is fundamentally flawed.

<!-- is there a good metaphor / image we can add here? -->

This isn't a problem we can ignore. We need a new type of organization, one built from the ground up for transparency, accountability, and alignment. Before we can align AI, we must first engineer *aligned organizations*. This is the proving ground.

### DAOs: A New Hope for Alignment

A promising candidate for this new type of organization is the **Decentralized Autonomous Organization (DAO)**. Unlike a traditional corporation, which operates through opaque hierarchies and private ledgers, a DAO operates on a blockchain. Its rules are encoded in smart contracts, its treasury is fully transparent, and its decisions are made by its community members.

This structure offers a potential solution to the alignment problem. But to explore that, we first need a formal definition of alignment in this context.

**Organizational Alignment:** An organization is aligned with its members to the degree that its actions and state minimize the deviation from the collective, aggregate beliefs of those members.

In simple terms, an aligned organization *does what its members actually want it to do*. The goal of DAO governance is to create a system of rules that achieves this alignment efficiently and robustly.

<!-- can we formalise this notion!? -->

A simple DAO might be a collective investment fund. Members contribute capital, receive voting tokens, and then vote on proposals like "Invest 10% of the treasury in Project X." The rules are clear, the treasury is visible to all, and the voting mechanism (e.g., 1-token-1-vote, quadratic voting) is programmed into the DAO's code. More advanced DAOs use these same principles to govern everything from software protocols to scientific research grants, often incorporating sophisticated mechanisms like delegating votes to trusted experts.

But do these systems actually produce alignment? Or do they create new problems of their own? To answer this, we need to move beyond intuition and build a model.

### A Framework for Analyzing Governance

To understand if DAOs can solve the alignment problem, we need a way to simulate them—a wind tunnel for governance. We can do this by representing the DAO and its members in a formal, mathematical framework.

**The Intuition:**

Imagine the "state" of a DAO—everything from its treasury allocation to its strategic priorities—as a single point in a high-dimensional space. To make this concrete for an AGI DAO, this vector could represent the data centers currently being used, the specific AI models being served, the level of funding for safety research, and the stringency of its ethical protocols.

Every member of the DAO also has a point in this space, representing their ideal state for the organization, reflecting their personal preference for how the DAO should be run. The goal of governance is to move the DAO's point through this space, via proposals, to a location that is as close as possible to the collective ideal of its members.

**A Formal Framework for DAO Governance Simulation**

#### I. Core Components (The "Environment")

*   **Agents (A):** A set of `n` agents, `A = {a_1, a_2, ..., a_n}`.
*   **State Space (S):** The state of the DAO is represented by a vector `S_t ∈ ℝ^k` at time `t`. `S_0` is the initial state.
*   **Beliefs (Y):** Each agent `a_i` has a static belief vector `y_i ∈ ℝ^k`. This represents their ideal state for the DAO.
*   **Tokens (X):** Each agent `a_i` holds `x_i` governance tokens. The total number of tokens is `X_total = Σx_i`. The distribution can be unequal (e.g., drawn from a power-law distribution).
*   **Proposals (P):** A set of proposals `P = {p_1, p_2, ..., p_m}`. Each proposal `p_j` is a delta vector `p_j ∈ ℝ^k`, representing a potential change to the DAO's state.

#### II. The Governance Process (The "Simulation Loop")

For each time step `t = 1, 2, ...`:

1.  **Proposal Generation:** A proposal `p_t` is introduced for voting.
    *   *Simplification:* `p_t` is drawn randomly from a predefined distribution (e.g., uniform random vectors within a certain magnitude).

2.  **Voting:** Each agent `a_i` evaluates `p_t` and decides to vote 'For' or 'Against'.
    *   **Agent's Goal:** An agent wants to move the DAO state `S_t` closer to their belief `y_i`.
    *   **Decision Heuristic:** Agent `a_i` votes 'For' if the proposed state `S_{t+1} = S_t + p_t` is "better" for them than the current state `S_t`. A simple way to model this is to check if the distance to their belief decreases:
        `||(S_t + p_t) - y_i|| < ||S_t - y_i||`
        (where `||.||` is the Euclidean distance).

3.  **Tallying:** The votes are tallied according to a chosen mechanism.
    *   *Baseline Mechanism (1-Token-1-Vote):*
        *   `Votes_For = Σ x_i` for all `a_i` who voted 'For'.
        *   `Votes_Against = Σ x_i` for all `a_i` who voted 'Against'.
    *   The proposal `p_t` is **accepted** if `Votes_For > Votes_Against`.

4.  **State Update:** The DAO's state is updated based on the outcome.
    *   If `p_t` is accepted, `S_{t+1} = S_t + p_t`.
    *   If `p_t` is rejected, `S_{t+1} = S_t`.
    *   *Imperfection Model (Imperfect Execution):* If accepted, the update could include noise: `S_{t+1} = S_t + p_t + ε`, where `ε` is a random noise vector.

(Note: This model assumes perfect execution and observation of the state. This is a reasonable starting point because, unlike traditional organizations, DAOs are transparent by default and can execute proposals via smart contract. We will return to these assumptions later.)

5.  **Utility/Loss Calculation:** Each agent measures their alignment.
    *   **Individual Loss `L_i(t)`:** The misalignment for agent `a_i` at time `t` is the distance between the current DAO state and their belief: `L_i(t) = ||S_t - y_i||`.
    *   **System-wide Misalignment `L_total(t)`:** The aggregate loss could be the average or sum of all individual losses.

### A Laboratory for Governance

This framework creates a computational laboratory where we can rigorously test the dynamics of governance and ask critical questions:

*   **Plutocracy:** What happens if one agent holds 50% of the tokens? Our model can simulate how this "whale" can drag the DAO's state vector towards their own belief, even if it misaligns with 99% of the other members.
*   **Voter Apathy:** We can model scenarios where only a fraction of agents participate in votes. This allows us to measure how apathy amplifies the power of whales and makes the system vulnerable to capture by small, highly-motivated groups.
*   **Gridlock:** What happens if the community's beliefs are polarized into two opposing clusters? The model can show how this leads to gridlock, where no proposal can gain a majority, leaving the DAO's state stuck far from anyone's ideal point.
* **Efficiency and Stability:** How quickly does a DAO converge on a stable state? Is that state robust, or can a small shock (like a controversial new proposal) easily dislodge it and send the system into chaos?
* **Minority Rights:** Can a minority group with very strong preferences on a specific issue ever win a vote, or are they always overruled by a largely indifferent majority? This helps test the fairness of different voting systems.
* **Path Dependency:** How much does the final state of the DAO depend on the order of the first few proposals? Can a few early decisions lock the organization into a suboptimal trajectory forever?

The Fundamental Limits of Governance
<!-- what are the fundamental 'limits' to / implications of any democratic system? -->

This framework doesn't just let us test specific mechanisms; it forces us to confront the fundamental limits of any democratic system.

- **A Product of its Voters:** The model makes it clear that a DAO's state is fundamentally a function of its members' beliefs. You cannot produce an outcome that is alien to the inputs. If the members' collective beliefs are short-sighted or misguided, the DAO's actions will be too. This illustrates the "garbage in, garbage out" principle of collective intelligence.

- **No Perfect System:** The work of economists like Kenneth Arrow has mathematically proven that no voting system can be perfect across all criteria (e.g., fairness, decisiveness, resistance to strategy). Our model allows us to visualize these trade-offs. A system that is decisive (like 1-token-1-vote) might be unfair to minorities. A system that is fairer (like quadratic voting) might be more prone to gridlock. There is no magic bullet, only engineering trade-offs.

<!-- TODO! -->

### Extending the Model to Real-World Complexity

The true power of this framework is its extensibility. We can easily swap out rules and add new mechanisms to see how they affect the outcome.

*   **Quadratic Voting (QV):** We can change the voting rule from "1-token-1-vote" to a QV model where voting power is the square root of tokens held. The simulation would then show how QV dramatically reduces the power of whales and forces them to build broader consensus, likely leading to a final state that is more representative of the average member's belief.
*   **Sub-DAOs and Staking:** We can model advanced, agile governance. A main DAO can vote to fund a "sub-DAO"—a specialized team tasked with a specific goal. To ensure alignment, the sub-DAO's leader must **stake** their own tokens as a bond against negligence or fraud. Our model can test the consequences: if the stake is slashed for any failure, leaders become risk-averse and do nothing. If it's only slashed for proven malice, and there are large token **bonuses** for good performance, we can find a "sweet spot" that encourages both accountability and ambitious, innovative work.

## Initial Analysis of the Model: The Weighted Median Theorem

Now, let's use the framework to derive our first concrete insight. What is the single most important factor determining where the DAO ends up?

Let's simplify the model to its absolute core:

- The state space is 1-dimensional (k=1). Imagine a single slider for the DAO's budget, from "0% to research" to "100% to research."
- The voting mechanism is 1-Token-1-Vote.

Now, consider a proposal p to move the state S slightly to the right (e.g., increase research funding by 1%). Who votes 'For' this proposal?

An agent a_i will vote 'For' if the new state S+p is closer to their ideal belief y_i. This means that everyone whose ideal point is to the right of the proposal's midpoint will vote 'For'.

The proposal will pass if the total token weight of the 'For' voters is greater than the 'Against' voters. This process will continue, with the DAO's state moving left or right, until it reaches an equilibrium point.

Where is that equilibrium? It's the point where any proposal to move left is rejected, and any proposal to move right is also rejected. This happens when the voting power is perfectly balanced on both sides. This point is not the token-weighted mean of all beliefs. It is the token-weighted median.

This is the location of the "median voter", but weighted by token holdings. It's the belief y_m such that 50% of the token power lies on one side of it and 50% on the other.

Why this is a powerful insight:

This immediately tells us that the system is not sensitive to the intensity of belief, only the distribution of voting power. An agent whose belief is at y=100 has the same influence on the final direction as an agent with the same token count whose belief is at y=51, if the median is at y=50. It also shows that the system is highly resistant to outliers; a whale with an extreme belief at y=0 can't pull the median much, whereas they could dramatically pull the mean.

This simple result—that a simple DAO converges on the weighted median, not the mean—shows the power of this framework. It allows us to move from vague intuitions to precise, testable hypotheses about how these complex systems behave.

***

This is the path forward. The challenge of building a truly aligned organization is an engineering problem, and we must approach it with engineering tools. By modeling, simulating, and testing these complex systems, we can discover the principles of robust, decentralized governance.