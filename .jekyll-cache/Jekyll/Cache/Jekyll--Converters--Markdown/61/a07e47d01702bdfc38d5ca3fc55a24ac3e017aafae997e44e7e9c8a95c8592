I"£<p>What can we find and where do they take us?</p>

<h1 id="linear-paths">Linear paths</h1>

<p>Linear paths are the nice ones, no nasty bumps in the wayâ€¦</p>

<h3 id="matrix-multiplication">Matrix multiplication</h3>

<p>It turns out that matrix multiplacation naturally sums over paths.</p>

\[\begin{align*}
y &amp;= ABx \\
&amp;= \begin{bmatrix}
a_{11} &amp; a_{12} \\
a_{21} &amp; a_{22} \\
\end{bmatrix}
\begin{bmatrix}
b_{11} &amp; b_{12} \\
b_{21} &amp; b_{22} \\
\end{bmatrix}
\begin{bmatrix}
x_{1} \\
x_{2} \\
\end{bmatrix} \\
&amp;=\begin{bmatrix}
a_{11}b_{11} + a_{12}b_{21} &amp; a_{11}b_{12} + a_{12}b_{22} \\
a_{21}b_{11} + a_{22}b_{21} &amp; a_{21}b_{12} + a_{22}b_{22} \\
\end{bmatrix}
\begin{bmatrix}
x_{1} \\
x_{2} \\
\end{bmatrix}
\end{align*}\]

<p><img src="/images/LinearPath.png" alt="" class="center-image" /></p>

<p>So as we can see, the top left entry of $AB_{1,1}=a_{11}b_{11} + a_{12}b_{21}$ is the sum of the two paths shown in orange.</p>

<p>The point is that linear neural networks can be viewed as a sum over independent paths.</p>

<!-- Interesting similarities to graph algoriths.
Sum-product. Verterbi?
Message passing. -->

<h2 id="non-linearities">Non-linearities</h2>

<p>Kawaguchi/Choromasnska make bad assumptions. That paths are independent, and ??</p>

\[y_{j,i} = \sum_{p=1}^{\mid P_j\mid} [X_i]_{(j,p)} [Z_i]_{(j,p)} \prod_{k=1}^{H+1} w_{k,p}^{(k)} \\\]

<h4 id="entanglement">Entanglement</h4>

<p>Entangled! $P(p_1,p_2) = P(p_1)P(p_2)$ in the linear case. But â€¦ (want a nice graphical model pic? like explaining away?)</p>

<p>The intuition is that the knowledge that:
  path $m$â€™s ith ReLU is not activated ($z_i &lt; 0$) tells us that other paths ReLU must not be activated.
  So the value of a path is entangled with the value of other paths.</p>

<h3 id="conjecture">Conjecture</h3>

<p><code class="language-plaintext highlighter-rouge">Deep -&gt; Wide transform</code></p>

<blockquote>
  <p>A deep network can be written as the sum over paths.
A sum over paths is a one layer network.</p>
</blockquote>

<hr />

<ol>
  <li><strong>Deep paths</strong>
    <ul>
      <li>Deep nets have deeper paths than wide nets.</li>
      <li>Increases in the depth of paths allow them to make higher order frequencies/oscillations. (represent more non linear fns)</li>
      <li>Higher order frequencies/oscillations give increased representational power (per path).</li>
    </ul>
  </li>
  <li><strong>Weight tying across paths</strong>
    <ul>
      <li>Deep nets have exponentially more paths than wide nets.</li>
      <li>Deep nets share parameters between the many paths.</li>
      <li>Weights with more shared paths can be learned faster. Weight tying across paths means each weight recieves more data.</li>
    </ul>
  </li>
</ol>

<p>Therefore: Deep networks are better than wide nets. They can learn more complex functions, in O(? size of net) vs O(? size of net), and can learn them faster, O(?? t) vs O(?? t) .</p>
:ET