I"‘<h1 id="supervised-learning-is-all-you-need-for-rl">Supervised learning is* all you need (for RL)</h1>

<p>*well we donâ€™t know that for certain. It just makes a good titleâ€¦</p>

<p>Not. Itâ€™s an argument of sufficiency, not necessity.</p>

<h2 id="unsupervised-learning">Unsupervised learning</h2>

<p>Before autoencoders and contrastive losses, there was; boltzman machines, clustering, self-organising maps, â€¦
But these didnt work well. Why not?</p>

<p>Itâ€™s not immediately clear that unsupervised learning can be formulated as a supervised learning problem.
Fundamentally, unsupervised learning lacks supervision. In unsupervised learning, we care about inferring patters / regularities in the data.
There is no teacher that can supervised: letting us know which patterns or regularities are â€˜correctâ€™.</p>

<p>Supervised learning coverges quicker.</p>

<h2 id="reinforcement-learning">Reinforcement learning</h2>

<p>Attempts at SL RL.</p>

<ul>
  <li>?</li>
  <li>upside down</li>
  <li>hindsight</li>
  <li>?</li>
</ul>

<h4 id="bc-and-dataset-optimisation">BC and dataset optimisation</h4>

<!-- #### Behavioural cloning

https://ml.berkeley.edu/blog/posts/bc/ -->

\[J(\theta)= \mathop{\mathbb E}_{\pi_{\theta}} \Big [R(\tau) \Big] \\
J'(\theta, q) = \mathop{\mathbb E}_{q} \Big [\frac{\pi(\tau)}{q(\tau)} R(\tau) \Big] \\
\theta^{* } = \mathop{\text{argmax}}_{\theta} J'(\theta, q) \\
q^{* } = \mathop{\text{argmax}}_{q} J'(\theta, q) \\\]

<p>But now. we want to find maxmax?</p>

<p>Where $q$ is the distribution over data.</p>

<ul>
  <li>Policy learns to copy the data. Learning a policy becomes a supervised learning problem.</li>
  <li>Need to collect a â€˜goodâ€™ dataset.</li>
</ul>

<p>https://bair.berkeley.edu/blog/2020/10/13/supervised-rl/</p>

<p>Havent we just moved the â€˜hardâ€™ oprimisation problem to constructing a good dataset?
Or is there actually some advantage of this framing?</p>

<h4 id="upside-down-reinforcement-learning">Upside-Down Reinforcement Learning</h4>

<p><a href="https://arxiv.org/pdf/1912.02877.pdf">https://arxiv.org/pdf/1912.02877.pdf</a></p>

<h4 id="hindsight-replay">Hindsight replayâ€¦?</h4>

<p>Hrmm. Not sure about this? Policy will attempt clone the best trajectories. Not act optimally?!<br />If there are two suboptimal trajectories (whose union could make an optimal trajectory), â€¦!?</p>

<p>How tight is this lower bound? Why is it not exact?</p>

<p>Want to understand. How is behavioural cloning on a â€˜goodâ€™ dataset, the same as maximising expected return?</p>

<p>Can RL be framed as a sueprvised problem? If not, why not?</p>

<hr />

<h4 id="iterated-supervised-learning">Iterated supervised learning</h4>

<p>But the supervised learning is really <i>iterated</i> supervised learning. Doesnâ€™t this suffer the same problems RL does?</p>

<p>Open quiestions</p>

<ul>
  <li>How much harder is iterated supervised learning than supervised learning?</li>
  <li>?</li>
</ul>
:ET