I"L<p>This setting is inspired by the problems faced in science, especially economics and medicine. There are some experiments that are not feasible, or legal, or would take extraordinary amounts of organisation and energy. For example, a randomised controlled trial of the unconditional benefit would require: many countries to spend $$$ on an unconditional benefit, which they may be unwilling to do (and obviously there is no way of running is as a blinded experiment).</p>

<p>At some point we will want to automate the application of statistics to economics and healthcare. Unsupervised (machine) learning is to pattern recognitio, as supervised (machine) learning is to predictive intelligence, as reinforcement (machine) learning is to actionable intelligence. We will want this automated statistics to be able to find algorithms that explore and do experiments! But… This has two problems. Safety, which is considered here. Want to build AI with a bias to simple cheap experiments.</p>

<h2 id="examples">Examples</h2>

<blockquote>
  <p>Want to do experiments on particle physics. The explorer could; organise and convince tens of countries to support the construct a hadron collider the size of a country. Or we could spend those resources on searching for a cheaper experiment, like the wakefield accelerator.</p>
</blockquote>

<p>Ok, this has an interesting property. Multiple ways to learn the same thing. We want to pick the cheapest…</p>

<blockquote>
  <p>What is the effect of the quality of local libraries on the national economy? To answer this we could: do an experiment on ~20 countries, varying the quality of the local libraries, controlling for potential confounders like, national policy, culture, … and measure the effect over 50 years. Or, we could approximate that experiment with …? A set of local experiments, stimulus-response, measure and correct for confounders and correlates, …?</p>
</blockquote>

<hr />

<p><strong>Hypothesis:</strong> Simple, local, exploration strategies will naturally emerge from optimising for novelty (and/or compression?). They are easier to learn and yield a higher rate of return.</p>

<p>The low hanging fruit!</p>

<h2 id="sample-complexity-of-options-effects">Sample complexity of option’s effects</h2>

<p>Some states are harder to reach (and stay there). This means they are harder to explore.</p>

<p>Have a set of options. Each option has an associated energy cost (could simply be the length…).</p>

<ul>
  <li>Write $\tau(\tau(s_t, a_i), a_j) \equiv \tau(s_t, [a_i, a_j])$.
    <ul>
      <li>When does $\tau(s_t, [a_1, a_2, a_3]) \neq \tau(s_t, [a_1, a_3, a_2])$?</li>
      <li>How hard is this to learn as $a_n$ increases?</li>
    </ul>
  </li>
  <li>Write $\Delta \tau(s_t, a_i) \equiv \tau(s_t, a_i)- s_t$.
    <ul>
      <li>When does $\Delta \tau(s_t, a_n) \neq \tau(s_t, [a_i, \dots, a_{i+n}]) - \tau(s_t, [a_{j+n}, \dots, a_{n-1}])$?</li>
      <li>How hard is this to learn as $a_n$ increases?</li>
    </ul>
  </li>
</ul>

<p>Where $a_i\in O$ are options (not just actions).</p>

<p>When is it efficient/possible to learn a complex/global function from simple/local actions/experiments?</p>

<p>Can have arbitrary (normalised?) functions that assign costs to options. What we are really interested in is not the absolute values of the costs, but how they scale with different types of problem.</p>

<p>Reasons a state can be hard to reach. (really we care about reaching a pair of states, so we can compare the difference. and do science)</p>

<ul>
  <li>trajectory is sensitive to noise (and few possible trajectories)</li>
  <li>trajectories are long (want thing to capture having to gather resources for a task. eg we can easily buy a new iphone, just walk down to the store. but that doesnt include the required effort to earn the captial - financial capital, social capital, intellectual capital, …)</li>
  <li>other agents are attempting to move the state elsewhere. (thus you need to compete/convince/kill/?)</li>
  <li>?</li>
</ul>

<h2 id="limited-capacity-gradient-estimation">Limited capacity gradient estimation</h2>

<p>The state space is large.
Pertubation
\(\begin{align}
y &amp;= f(x) \\
\frac{df}{dx} &amp;= f(P(x)) \\
P(x) &amp;= x + \Delta x \tag{pertubation} \\
??? &amp;= f(C(x))) \tag{control}\\
\end{align}\)</p>

<p>Could simply measure the number of dimensions perturbed or controlled. Energy, $E=\mid P \mid + \mid C \mid$.</p>

<p>Related to <a href="https://papers.nips.cc/paper/7230-on-blackbox-backpropagation-and-jacobian-sensing.pdf">Blackbox bprop and jacobian sensing</a>.</p>

<p>Have some finite/limited capacity and/or attention to;</p>
<ul>
  <li>apply preturbations and controls. Can weakly control many things, or strongly control few dimensions.</li>
  <li>measure outcomes. Can accurately measure few outputs, or inaccurately measure many.</li>
</ul>

<p>(I am imagining a juggler trying to juggle 100 balls, or one of those music men buskers wih ~10 instruments).</p>

<p>This gives a kind of local-global tradeoff. (relationship to fourier transform and uncertainty principle)</p>

<p>The limits of limited capacity.</p>

<ul>
  <li>Measure and correct for confounders. We cannot measure everything during a single experiment… We are not sure if there is another hidden variable that is confounding our experiment.</li>
  <li>Control is only necessary when correlated? For other independent variables we would only control for efficiency? Could still estimate, but would want to reduce variance.</li>
</ul>

<hr />

<p>Rather than global perturbations being a linear fn of the number of pertubations made, there is some higher order function that calculates the cost of many pertubations.</p>

<hr />

<ul>
  <li>What about time-memory complexity? (to explore efficiently there is a fundamental tradeoff between time/samples and memory. Need to remember where I have been if I want to avoid repeating it…)</li>
</ul>
:ET