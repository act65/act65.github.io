I"Á'<p>Linear algebra and arrays combine to give us a powerful language. Letâ€™s explore the power of them represent interesting languages/systems/grammars/algebrasâ€¦</p>

<h2 id="embedded-linear-operators">Embedded linear operators</h2>

<p>One of the cool things about linear algebra is that linear functions can be written as arrays. This representational duality between operator and operand is rather elegant.</p>

<!-- Related to Turing's duality of data and program. Where ... -->

<!-- But are these more that just nice mathematical curiosities? Why is this useful or important? -->

<p>Let start with linear functions of polynomials.</p>

<p>Consider the space of polynomial coefficients, $a \in  {\mathbb R}^{\infty}$. Where $a_i$ is the coefficient for $x^i$. Thus $a = [0, 5, 3, 0, 1, \dots]^T$ represents</p>

\[0\cdot x^0 + 5\cdot x^1 + 3\cdot x^2 + 0\cdot x^3 + 1\cdot x^4 \\
= x^4 + 3\cdot x^2 + 5\cdot x\]

<h5 id="the-differentiation-operator">The differentiation operator</h5>

<p>We can define a differentiation operator, $D$, as a matrix.</p>

\[\begin{align*}
\mathcal D &amp;= \begin{bmatrix}
0 &amp; 1 &amp; 0 &amp; 0 &amp; \dots \\
0 &amp; 0 &amp; 2 &amp; 0 &amp;  \dots \\
0 &amp;0 &amp; 0 &amp; 3 &amp; \dots \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots \\
\end{bmatrix} \\
\end{align*}\]

<p>And we can â€˜doâ€™ differentiation via matrix multiplying $\frac{d}{dx}a = \mathcal D \cdot a$.</p>
<side>Problem (we need infinite matrices for this to work out...)</side>

<p>This all works out because differentiation is a linear function! Each column is the derivative of a â€˜basisâ€™ function, which in this case is each $x^n$. Now, lets test it out.</p>

\[\begin{align}
y &amp;= x^4 + 3\cdot x^2 + 5\cdot x \\
a &amp;= [0, 5, 3, 0, 1]^T \\
\frac{d}{dx} a &amp;= \mathcal D \cdot a \\
&amp;= \begin{bmatrix}
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 3 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 4 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
\end{bmatrix}
\begin{bmatrix} 0 \\ 5 \\ 3 \\ 0 \\ 1
\end{bmatrix} \\
&amp;= \begin{bmatrix} 5 \\ 2 \cdot 3 \\ 0 \\ 4 \cdot 1 \\ 0 \end{bmatrix} \\
\frac{d}{dx}a &amp;= 4\cdot x^3 + 6\cdot x + 5 \\
\end{align}\]

<side>Homework: What is the derivative of a derivative, $D \cdot D$? Does it make sense?</side>
<p>Awesome.</p>

<h4 id="integral-of-polynomials">Integral (of polynomials)</h4>

<p>What about integrals, they are linear operators. Can we write them in a similar manner?</p>

\[\begin{align}
\mathcal I &amp;= \begin{bmatrix}
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \dots \\
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \dots \\
0 &amp; \frac{1}{2} &amp; 0 &amp; 0 &amp; 0 &amp; \dots \\
0 &amp; 0 &amp; \frac{1}{3} &amp; 0 &amp; 0 &amp; \dots \\
0 &amp; 0 &amp; 0 &amp; \frac{1}{4} &amp; 0 &amp; \dots \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots \\
\end{bmatrix} \\
\end{align}\]

<p>So, we can define an integral operator, $\mathcal I$, and apply it via matrix multiplication.</p>

\[\begin{align}
y &amp;= x^4 + 3\cdot x^2 + 5\cdot x \\
a &amp;= [0, 5, 3, 0, 1]^T \\
\int a \; dx &amp;= \mathcal I \cdot a \\
&amp;= \begin{bmatrix}
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; \frac{1}{2} &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; \frac{1}{3} &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; \frac{1}{4} &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; \frac{1}{5} &amp; 0 \\
\end{bmatrix}
\begin{bmatrix} 0 \\ 5 \\ 3 \\ 0 \\ 1 \\ 0
\end{bmatrix} \\
&amp;= \begin{bmatrix} 0 \\ 0 \\ \frac{5}{2} \\ \frac{3}{3} \\ 0 \\ \frac{1}{5} \end{bmatrix} \\
&amp;=  \frac{1}{5} x^5 + \frac{3}{3} x^3 + \frac{5}{2} x^2\\
\int a \; dx &amp;=  \frac{1}{5} x^5 + \frac{3}{3} x^3 + \frac{5}{2} x^2 + c\\
\end{align}\]

<side>Ok. So this works, up to an added constant, $c$.</side>

<hr />

<p>Intriguingly, we can relate the integration and differentiation operators via $\mathcal I = \frac{1}{\mathcal D^T}$.</p>

<h3 id="bilinear-operations">Bilinear operations</h3>

<p>\(C_{ij} = \sum_k A_{ik}B_{kj} \\
C_{ijk} =\)</p>

<blockquote>
  <p>The matrix multiplication tensor
https://gist.github.com/act65/f956cc1ce73aca4fe435f225f8970ac4</p>
</blockquote>

<h2 id="algebras">Algebras</h2>

<p>And Type-systems (aka algebras?)
Strongly typed â€¦</p>

<h5 id="dual-numbers">Dual numbers</h5>

<blockquote>
  <p>the dual numbers extend the real numbers by adjoining one new element $\epsilon$ with the property $\epsilon^2 = 0$.</p>
</blockquote>

\[a + b\epsilon \rightarrow
\begin{bmatrix}
a &amp; b \\
0 &amp; a \\
\end{bmatrix} \\\]

\[\begin{align*}
(a + b\epsilon)(c + d\epsilon) &amp;= ac + ad\epsilon + bc\epsilon + bd\epsilon^2\\
&amp;= ac + (ad + bc)\epsilon \\
\begin{bmatrix}
  a &amp; b \\
  0 &amp; a \\
\end{bmatrix}
\begin{bmatrix}
  c &amp; d \\
  0 &amp; c \\
\end{bmatrix}
&amp;=
\begin{bmatrix}
  ac &amp; ad + bc \\
  0 &amp; ac \\
\end{bmatrix} \\
&amp;\rightarrow  ac + (ad + bc)\epsilon
\end{align*} \\\]

<p>Dual numbers actually happen to capture the product, sum and chain rules.!!! <strong>TODO</strong>.</p>

<h5 id="complex-numbers">Complex numbers</h5>

\[\begin{align}
1 &amp;\equiv \begin{bmatrix}
1 &amp; 0 \\
0 &amp; 1 \\
\end{bmatrix},\quad
i \equiv \begin{bmatrix}
0 &amp; 1 \\
-1 &amp; 0 \\
\end{bmatrix} \\
a + b i &amp;=
a\begin{bmatrix}
1 &amp; 0 \\
0 &amp; 1 \\
\end{bmatrix} +
b\begin{bmatrix}
0 &amp; 1 \\
-1 &amp; 0 \\
\end{bmatrix} \\
&amp;= \begin{bmatrix}
a &amp; b \\
-b &amp; a \\
\end{bmatrix}
\end{align}\]

\[\begin{align}
(a + b i)(c + d i) &amp;= ac + adi + bci - bd \\
&amp;= ac - bd + (ad - bc)i \\
\begin{bmatrix}
a &amp; b \\
-b &amp; a \\
\end{bmatrix}
\begin{bmatrix}
c &amp; d \\
-d &amp; c \\
\end{bmatrix} &amp;=
\begin{bmatrix}
ac - bd &amp;  ad - bc\\
-(ad - bc) &amp; ac - bd \\
\end{bmatrix}\\
&amp;= (ac - bd)\begin{bmatrix}
1 &amp; 0 \\
0 &amp; 1 \\
\end{bmatrix} +
(ad - bc)\begin{bmatrix}
0 &amp; 1 \\
-1 &amp; 0 \\
\end{bmatrix}
\end{align}\]

<ul>
  <li><strong>Q</strong> What does the fact that complex numbers can be represented as a 2 variable, 2x2 matrix? What does it imply about the types of transforms it can do?</li>
</ul>

<h4 id="quarternions">Quarternions</h4>

\[\begin{align}
1 &amp;= \begin{bmatrix}
\;1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; \;1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; \;1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; \;1 \\
\end{bmatrix},\quad
i = \begin{bmatrix}
0 &amp; -1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; -1 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
\end{bmatrix} \\
j &amp;= \begin{bmatrix}
0 &amp; 0 &amp; -1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; -1 &amp; 0 &amp; 0 \\
\end{bmatrix},\quad
k = \begin{bmatrix}
0 &amp; 0 &amp; 0 &amp; -1 \\
0 &amp; 0 &amp; -1 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 0 &amp; 0 \\
\end{bmatrix} \\
\end{align}\]

<p>and the negatives, $-1, -i, -j, k$ are simple derived by taking the element wise negative of the linear representations.</p>

<side>Feel free to verify that (say) $k \cdot j = -i$</side>

\[\begin{align}
i \cdot j &amp;= \begin{bmatrix}
0 &amp; -1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; -1 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
\end{bmatrix} \cdot
\begin{bmatrix}
0 &amp; 0 &amp; -1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; -1 &amp; 0 &amp; 0 \\
\end{bmatrix} \\
&amp;= \begin{bmatrix}
0 &amp; 0 &amp; 0 &amp; -1 \\
0 &amp; 0 &amp; -1 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 0 &amp; 0 \\
\end{bmatrix} \\
&amp;= k \\
\end{align}\]

<p>The really cool part is that the quarternions, $\mathbb Q$, can be generated by taking the outer product of the complex numbers with themselves, $\mathbb C \otimes \mathbb C$. And this still works for these linear representations.</p>

\[\begin{align}
1_C \otimes 1_C &amp;= 1_Q \\
1_C \otimes i_C &amp;= i_Q \\
i_C \otimes i_C &amp;= j_Q \\
i_C \otimes 1_C &amp;= k_Q \\
\end{align}\]

<p>More info <a href="https://groupprops.subwiki.org/wiki/Linear_representation_theory_of_quaternion_group">here</a></p>

<h4 id="octonions">Octonions</h4>

<p>The pattern continues!</p>

<hr />

<ul>
  <li>Clifford algebras?</li>
  <li>?</li>
</ul>

<h3 id="computation">Computation</h3>
<h5 id="logic">Logic</h5>

<p>We have two bits, $xy$. We can encode the 4 possible arangements $00, 01, 10, 11$ in a four dimensional space, $\mathbb R^4$. Let $a \in \mathbb R^4$, then $a_0 = a(00), a_1 = a(01), a_2 = a(10), a_3 = a(11)$. Now we require all operations to be unitary and all vectors to be within the basis, $a_0, â€¦, a_n$.</p>

\[\begin{align}
CNOT \equiv
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
\end{bmatrix}
\end{align}\]

<h4 id="quantum-computation">(quantum) computation</h4>

<h3 id="memory">Memory</h3>

<p><a href="http://www2.fiit.stuba.sk/~kvasnicka/CognitiveScience/6.prednaska/plate.ieee95.pdf">Holographic reduced representations</a></p>

<h3 id="graphs">Graphs</h3>
<p>(<code class="language-plaintext highlighter-rouge">matmul -&gt; message passing</code>)</p>

<p>!?? Can represent graphs in linalg.</p>

<p>Other</p>

<ul>
  <li>Seripinski triangle</li>
</ul>

<h1 id="thoughts">Thoughts</h1>

<ul>
  <li>Why is linear algebra such an effective language for thinking about and expressing structure in systems?</li>
  <li>What makes it so good? Is it really that great?</li>
  <li>The duality between representations and functions. A (linear) function can be written as a matrix. <strong>Q</strong> For what other classes of function can we find dual representations?</li>
  <li>What alternatives are there to representing information that rival linear algebra? Other data structures, trees, graphs, !??</li>
  <li>in some representations, matmul ends up capturing another operation in the original representation. other times, the arguments capture the operations.</li>
</ul>

<!--
- could talk about "Concrete models and empirical evaluations for the categorical compositional distributional model of meaning". -->
:ET