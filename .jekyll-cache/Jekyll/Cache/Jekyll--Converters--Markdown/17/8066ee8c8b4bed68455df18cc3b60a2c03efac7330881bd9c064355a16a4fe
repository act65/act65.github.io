I"0<p>What is the difference between credit assignment and causal inference?</p>

<ul>
  <li>Eligibility traces -&gt; gradient of policy. + Kickback + !?</li>
  <li>Differentiation $\neq$ credit assignment. Credit assignment is a counterfactual notion. Differentiation has no concept of priors, and what might have happened if a parameter was a different value. Or does it!?</li>
  <li>Alternative ways to assign credit!?</li>
  <li>Why we need stochastic variables for discrete credit assignment.</li>
  <li>!?</li>
</ul>

<p>If your fault.</p>

<p>Legal.
Nesessary and sufficient causation.</p>

<p>Only want to assign credit to causes, not correlates?!</p>

<h2 id="eligibility-traces">Eligibility traces</h2>

<p>Given the typical temporal difference update (for a tabular representation) we could augment it with a â€˜traceâ€™, $e_t(s_t, a_t)$.
\(\begin{align}
Q_{t+1} &amp;= Q_t + \alpha \delta_t e_t \\
\Delta Q &amp;= \alpha \delta_t e_t \\
e_t[s, a] &amp;=
\gamma \lambda e_{t-1}[s, a] + \mathbf 1_{s=s_t, a=a_t}\\
\end{align}\)</p>

<p>What is this â€˜traceâ€™ doing? Well, itâ€™s an exponentially decaying variable, but for a given state-action pair, if they are used at time t, we reset the â€˜traceâ€™ to one.</p>

<p>We can interpret this as keeping a (decaying) memory of the state-action pair, or is exponentially decaying recency. Thus a trace.</p>

<hr />

<p>But what if we want to use some sort of function approximation to represent $Q(s_t, a_t)$, rather than tables? We can generalise the definition above to;</p>

\[\begin{align}
e_t(s_t, a_t) &amp;= \gamma \lambda e_{t-1}(s_t, a_t) + \nabla_{\theta_t} Q_t(s_t, a_t)\\
\end{align}\]

<p>This generalised definition recovers the tabular case, as $\nabla_{\theta_t} Q_t[s_t, a_t] = \mathbf 1_{s=s_t, a=a_t}$.</p>

<p>However, we have introduced a new problem. We need to update all of the state-action pairs. In the tabular case, this was possible (but maybe not a good idea), as we could iterate through all the distinct state-action pairs. But how can we efficiently make this update for an infinite number of state-action pairs?</p>

<hr />

<p>A quick aside.</p>

<p>What are we doing here? What is $\nabla_{\theta_t} Q_t(s_t, a_t)$?</p>

<p>We are keeping an exponential average of the gradient of the state-action value with respect to each parameter. Which tells us: how did each parameter contribute to the estimated value. That feels a lot like assigning credit to different parameters.</p>

<p>Forward vs backward views. Why are these attractive? Keeps the trace of a trajectory lingering. Could simply use the trajectory, but it needs to be feed in, thus we need to store the obs and action. Rather the trace and be stored internally, (hopefully more cheaply!?).</p>

<p>https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2007-49.pdf</p>

<hr />

<p>Ok, so is there a difference to momentum?
https://distill.pub/2017/momentum/</p>

\[\begin{align}
\frac{\partial L}{\partial \theta_t} = \frac{\partial L}{\partial Q} \frac{\partial Q}{\partial \theta}
\end{align}\]

<p>A kind of momentum?</p>

<hr />

<p>Integrated gradients = causal effect = eligibility trace?</p>

<p>https://arxiv.org/abs/1703.01365</p>

<hr />

<p>Resources</p>

<ul>
  <li>http://incompleteideas.net/book/ebook/node72.html</li>
  <li>http://pierrelucbacon.com/</li>
  <li>http://web.eecs.umich.edu/~baveja/Papers/ICML98_LS.pdf</li>
  <li>https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2007-49.pdf</li>
  <li>https://amreis.github.io/ml/reinf-learn/2017/11/02/reinforcement-learning-eligibility-traces.html</li>
  <li>http://rl.cs.mcgill.ca/tracesgroup/</li>
</ul>
:ET