I"½<p>Imagine you have a optimisation function which you use to find good minima of other functions.</p>

<p>I want to know the best way to optimise a given (type of) function, e.g. CNNs. How can I do this? Optimise the optimiser?</p>

<p>So this is also know as hyperparameter optimisation or meta-learning. The techniques used are â€¦ grid/random searches â€¦???</p>

<p>Instead, could we use the optimiser we already have? Afterall, optimisation is just a function. We have a loss function, the number of steps required to get to a minima, or the â€¦</p>

<h3 id="verson-01">Verson 0.1</h3>

<p>For simplicity we consider SGD as a parameterised function (parameterised by the learning rate).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#A simple function to be optimised
</span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">*</span><span class="mi">2</span>

<span class="c1">#A simple convex optimiser with a single parameter
</span><span class="k">def</span> <span class="nf">optimise</span><span class="p">(</span><span class="n">func</span><span class="p">,</span><span class="n">params</span><span class="p">):</span>
    <span class="k">while</span> <span class="n">x</span><span class="p">[</span><span class="n">steps</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="n">steps</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">tolerance</span><span class="p">:</span>
        <span class="n">x</span><span class="p">[</span><span class="n">steps</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">steps</span><span class="p">]</span> <span class="o">-</span> <span class="n">params</span> <span class="o">*</span> <span class="n">grad</span><span class="p">(</span><span class="n">func</span><span class="p">,</span><span class="n">x</span><span class="p">[</span><span class="n">steps</span><span class="p">])</span>
        <span class="n">steps</span> <span class="o">+=</span><span class="mi">1</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span><span class="n">steps</span>

<span class="c1">#Recursively call the optimiser on itself
</span><span class="k">def</span> <span class="nf">recusrive_optimiser</span><span class="p">(</span><span class="n">func</span><span class="p">,</span><span class="n">params</span><span class="p">,</span><span class="n">depth</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">depth</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">optimise</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">params</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">optimise</span><span class="p">(</span><span class="n">recursive_optimiser</span><span class="p">,</span><span class="n">params</span><span class="p">,</span><span class="n">depth</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">*</span><span class="mi">4</span>

<span class="n">min_f</span><span class="p">,</span><span class="n">params</span> <span class="o">=</span> <span class="n">recursive_optimiser</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">init</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">min_g</span><span class="p">,</span><span class="n">params</span> <span class="o">=</span> <span class="n">recursive_optimiser</span><span class="p">(</span><span class="n">g</span><span class="p">,</span><span class="n">params</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>Wont this result in all optimisers (with a call) having the same parameters?</li>
  <li>So weight tying across optimisers, how does this make sense?</li>
  <li>What should I be doing with the outputs of the optimisers, while deep in the recursion?</li>
  <li>How is this related to data points and gradients evaluated at a point?</li>
  <li>The point is that it optimises a function as well as itself (to optimise the function faster).</li>
  <li>What does depth mean here? I get what depth two means, optimise the optimiser. But what does more depth do?</li>
  <li>What relation is there to higher order moments or second order methods?</li>
</ul>

<h3 id="verson-02">Verson 0.2</h3>

<p>Just taking one step in the right direction from data. So the optimier should look at how much the step taken reduced the loss function and seek to maximise the reduction? Could this lead to poor places?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#A simple function to be optimised
</span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">*</span><span class="mi">2</span>

<span class="c1">#Recursively call the optimiser on itself
</span><span class="k">def</span> <span class="nf">recusrive_optimiser</span><span class="p">(</span><span class="n">func</span><span class="p">,</span><span class="n">params</span><span class="p">,</span><span class="n">depth</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">depth</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">-</span> <span class="n">params</span> <span class="o">*</span> <span class="n">grad</span><span class="p">(</span><span class="n">func</span><span class="p">,</span><span class="n">x</span><span class="p">[</span><span class="n">steps</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span>

</code></pre></div></div>

<blockquote>
  <p><i>I see 3 potential challenges with the recurrent setup, but i guess they all can be solved somehow:</i></p>
  <ol>
    <li>Gradient for the task at hand and the optimizer have different distributions. This may be a problem, especially if the gradfients in one of the problems are much bigger (the optimizer would then just disregard the other task almost completly).</li>
    <li>Different coordinates of lstm optimizers also have different distributions of gradient (bias vs connections, different lstm layers). This was a problem we encountered on cifar and we had to use 2 separator optimizers to mitigate it.</li>
    <li>I guess that the optimizer could easily diverge because of the recurrent relationship if you donâ€™t entourage stability somehow (L2 penalty?).&lt;/i&gt;</li>
  </ol>
</blockquote>

<p>Some sort of batch normalisation of gradients seems like it could help with 1.? Nope. Where are the batchesâ€¦?</p>
:ET