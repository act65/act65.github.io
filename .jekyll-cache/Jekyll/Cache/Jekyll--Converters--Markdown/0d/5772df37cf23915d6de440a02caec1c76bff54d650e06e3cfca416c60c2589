I"i<p>We understand the gradient descent operator reasonably well.
We can characterise easy (convex) and hard (non-convex) problems.
We can â€¦? rates of convergence.
Indctive bias.</p>

\[x \leftarrow \text{GD}(x) \\
x_{t+1} = x_t -\eta \nabla L(x)\]

<p>What about the natural selection operator?
\(x \leftarrow \text{NS}(x) \\
x_{t+1} \sim L(x_t)\)</p>

<ul>
  <li>Sample according to fitness.</li>
  <li>Replicate and possibly mutate</li>
</ul>

<hr />
<p>So.</p>

<ul>
  <li>Which classes of problems are easy for NS?</li>
  <li>How quickly does it converge?</li>
  <li>What inductive bias does it have?</li>
</ul>
:ET