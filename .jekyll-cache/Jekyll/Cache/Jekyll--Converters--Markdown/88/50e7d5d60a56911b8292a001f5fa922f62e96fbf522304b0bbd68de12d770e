I"<p>Why do we discount future rewards? What priors does it imply?
Should discounting be a function of uncertainty?</p>

<h3 id="machine-behaviour">Machine behaviour</h3>

<p>Lets just build some intuition for what the discount does, what it controls, etc.</p>

<p>Define depression as:
Define succeptibility to addiction (/ impulsiveness) as:</p>

<p>Want some intuitive examples</p>

<h2 id="discounting">Discounting</h2>

<p><strong>Q:</strong> Why do we discount? Could use the average reward.</p>

<p><strong>TODO</strong> Want to derive the need for a discount when there is noise within the model used by a planner.</p>

<h3 id="discounting-for-uncertainty">Discounting for uncertainty</h3>

\[\begin{align}
Q(s_t, a_t) &amp;= r(s_t, a_t) + \gamma \mathop{\mathbb E}^{s' \sim P(\cdot | s_t, a_t)}_{a\sim \pi(\cdot| s')} Q(s',a)\\
Q(s_t, a_t) &amp;= r(s_t, a_t) + \gamma \sum_{s'}P(s'|s_t, a_t)\sum_a \pi(a | s') Q(s',a)\\
\end{align}\]

<h5 id="policy-entropy">Policy entropy</h5>

\[\begin{align}
Q(s_t, a_t) &amp;= r(s_t, a_t) +  \sum_{s'}P(s'|s_t, a_t) \gamma(s_t)\sum_a \pi(a | s') Q(s',a)\\
\gamma(s) &amp;= 1-\mathop{\mathbb E}_{a\sim\pi(\cdot | s)}[-\log(\pi(a | s))]\\
\end{align}\]

<p>For many possible actions, this could be expensive to calculate at every step.</p>

<p>Can this still be solved as a set of linear equations?</p>

<p>Take a minimum entropy policy, $H(\pi) = 0$, then $\gamma = 1$. This is a deterministic policy, yielding zero â€¦ making it far easier to plan.</p>

<h5 id="transition-entropy">Transition entropy</h5>

\[\begin{align}
Q(s_t, a_t) &amp;= r(s_t, a_t) +  \gamma(s_t, a_t)\sum_{s'}P(s'|s_t, a_t) \sum_a \pi(a | s') Q(s',a)\\
\gamma(s, a) &amp;= 1-\mathop{\mathbb E}_{a\sim P(\cdot | s, a)}[-\log(P(s' | s, a))]\\
\end{align}\]

<h2 id="discount-rates">Discount rates</h2>

<p>Why exponential discounting?</p>

<p>How to deal with uncertainty?</p>

<h2 id="uncertain-transitions">Uncertain transitions.</h2>

<p>We should only get less certain about future states.</p>

\[H(s_t) \le H(s_{t+1}) \\\]

<p>Want to enforce this prior.</p>

<ul>
  <li>In the tabular case, this naturally occurs as a function of the row normalisation.</li>
  <li>In a function approximation case, how can we enforce this prior?</li>
</ul>

\[R(s_t, a_t) = \big | \log(\frac{H(\tau(s_t, a_t))}{H(s_t)} ) \big | \\\]

<p>Properties.</p>
<ul>
  <li>Asymmetric. Care a lot if $H(s_t) \le H(s_{t+1})$ but not so worried about $H(s_t) \ge H(s_{t+1})$.</li>
  <li>?</li>
</ul>

<hr />

<ul>
  <li><a href="https://openreview.net/forum?id=rkezdaEtvH">Hyperbolic Discounting and Learning Over Multiple Horizons</a></li>
  <li><a href="https://arxiv.org/abs/1910.02140">Discounted Reinforcement Learning Is Not an Optimization Problem</a></li>
</ul>

<hr />

<p>What about a discount greater than 1?</p>
:ET