I"—<p>I want to know; how does the generalisation error of my classifier scale as a function of the information in my prior?</p>

<h2 id="background">Background</h2>

<p>Generalisation is only possible when there is some relationship between the models generating training and testing data.</p>

<p>In the simplest case the relationship is the identity, thus training and testing are the same.</p>

<p>Next is where they have the same model, but are sampled under different distributions.</p>

<!--
Also.
- Diff model + same dist.
- Diff model + diff dist.
(do we ever care about these cases?!)
-->

<p>Have some prior over the relationship between  $P(\theta_{test},\theta_{train})$.</p>

<!--
When can we expect generalisation!?
Given problem X, priors P, data D.
What is the test accuracy likely to be?
-->

<p>Other examples;</p>

<ul>
  <li>time series. initial condition sampled from same distribution. same dynamical system.</li>
</ul>

<h4 id="nns">NNs</h4>

<p>A NN + SGD has many build in priors.
Consider the function space realisable by a neural network. Training via SGD prefers some functions over others, despite the fact that, on the observed data they perform the same.</p>

<!--
More correctly?? it prefer some weight settings over others, which yields a preference over functions.
-->

<table>
  <tbody>
    <tr>
      <td>Thus SGD gives a prior over functions $P(f)$. And we want to know $P(f</td>
      <td>D)$.</td>
    </tr>
  </tbody>
</table>

<h2 id="motivation">Motivation</h2>

<p>Sample complexity captures the data-dependent generalisation (somewhat akin to interpolation). For exampleâ€¦</p>

<p>Now, consider generalisation between two different training distributions $P, Q$ where the supports do not intersect. More data from $P$ does not help us learn anything about $Q$. So a sample complexity analysis will not help.</p>

<p>Rather, to understand the effects of our models and algorithms (eg NNs + SGD) on generalisation we need to understand how their priors (or inductive biases, whether implicit or explicit) effect sample complexity.</p>

<!--
for starters, let's just consider how generalisation accuracy scales with prior info.

ultimate goal, how does generalisation accuracy with prior info and more data.
given these priors, what is the sample complexit?
give other priors, what is the sample complexity?
-->

<p>Question we would like to be able to answer;</p>

<ul>
  <li>
    <h2 id="if-i-have-a-budget-of-samples-then-how-much-info-does-my-prior-need-to-provide-can-we-use-this-to-filter-potential-priors">If I have a budget of samples, then how much info does my prior need to provide? Can we use this to filter potential priors.</h2>
  </li>
</ul>

<h2 id="formal-setting">Formal setting</h2>

<table>
  <tbody>
    <tr>
      <td>Sample a dataset $D_{train} \sim P(x, y</td>
      <td>\varphi)P(\varphi)$ from some training distribution.</td>
    </tr>
  </tbody>
</table>

<p>Evaluate on some testing distribution $P_{test}(x, y)$.</p>

\[\mathcal L(\theta_t) = \mathop{\mathbb E}_{x, y \sim P_{test}}[\ell(f(x, \theta_t), y)]\]

<p>Find the optimal parameters given some prior $P(\theta)$</p>

\[\begin{align*}
P(\theta | D_{train}) = \frac{P(D_{train} | \theta) P(\theta)}{P(D)} \\
P(\theta | D_{train}) = \frac{P(D_{train} | \theta) P(\theta)}{\int P(D | \theta) P(\theta)d\theta} \\
\end{align*}\]

<h2 id="example">Example</h2>

<p>???</p>

<h2 id="quantifying-a-prior">Quantifying a prior</h2>

<!--
Let $i = \text{MI}(P(\theta) ; P_{test}(x, y))$.
As $i$ increases, how does
-->

<p>$i = \mathcal H(P(\theta))$.</p>

<p>Properties</p>
<ul>
  <li>If $i=0$ then we have an uninformative prior.</li>
  <li>
    <h2 id="in-the-limit-of-i-to-infty-we-can-specify-theta-">In the limit of $i \to \infty$ we can specify $\theta^{* }$.</h2>
  </li>
</ul>

<p>But should the amount of info be relative to the true distribution!?</p>

<p>So an alternativeversion of sample complexity should reduce the entropy of the posterior
$\mathcal H(P(\theta | D_n))$.</p>

<p>But
\(\begin{align*}
\mathcal H(P(f | D_n)) &amp;= - \int P(f | D_n) \ln P(f | D_n) df \\
&amp;= - \int \frac{P(D_n | f) P(f)}{P(D_n)} \ln \Big( \frac{P(D_n | f) P(f)}{P(D_n)}  \Big) df \\
P(D_n) &amp;= \int P(D_n|f)P(f) df \\
\end{align*}\)</p>

<p>If we pick a prior P(f) as the</p>

\[\begin{align*}
&amp;\mathop{\text{min}}_{P} KL(P(f) \parallel P(f)) \\
&amp;\text{s.t.} H(P(f)) \ge \alpha \\
&amp;\mathop{\text{min}}_{P} (1-\alpha) KL(P(f) \parallel P(f)) + \alpha H(P(f))
\end{align*}\]

<p>(Does this really pick the best prior?? For each given limit of info?)</p>

<hr />

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Sample complexity: $\mathcal H(P(f</td>
          <td>D_n)) = \mathcal O(n)$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Prior complexity: $\mathcal H(P(f</td>
          <td>D_n)) = \mathcal O(\alpha)$</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<!--
What are alternative frameworks for working with priors?
- Convergence rates given a regulariser
- Mutual info??
- ?
 -->

<p>&lt;!â€“
What if bad priors are picked? Then we can have a prior with low entropy, but â€¦</p>

<p>There exists a prior with entropy less than H that allows efficient learning.
  â€“&gt;</p>

<!--
Can we evaluate P(D|f) as the population loss??
Maybe exponentiated? Why not?
 -->

<h2 id="notes">Notes</h2>

<ul>
  <li>I am expecting non-linear (â€˜phase transitionsâ€™) when there is enuogh information to convey some prior which them makes the problem scale better with the data. ??? really. would incrementally more info not necessarily help?</li>
</ul>

<h2 id="refs">Refs</h2>

<ul>
  <li>Bayes nets</li>
  <li>
    <h2 id="pac">PAC</h2>
  </li>
</ul>
:ET