I"q<p>Inductive bias in exploration.</p>

<p>What priors could make sense in exploration?</p>

<p>Want to explore first;</p>
<ul>
  <li>states that we can control,</li>
  <li>states that allow us to control many other dimensions of the state</li>
  <li>states that behave the most unpredictably (have large variance).
or the opposite? states that we can be certain about.</li>
  <li>state-actions with large effect. a large change in state</li>
  <li>actions with disentangled effects (leaving other state dimension invariant)</li>
  <li>states / actions that allow efficient traversals. (fewer actions required to get from A to B)</li>
  <li>find actions that have minimal, yet predictable effect (for reductionism).</li>
</ul>

<hr />

<p>Surprise has the inductive bias that it wants to explore noisy states first. The nosier the better, as they are the least predictable.
This doesnt seem like a useful inductive biasâ€¦</p>

<hr />

<p>exploration strategies</p>

<ul>
  <li>optimisim in the face of uncertainty</li>
  <li>intrinsic motivation
    <ul>
      <li>surprise (rewarded for seeing unpredictable states)</li>
      <li>novelty (rewarded for seeing new states)</li>
    </ul>
  </li>
  <li>count based
    <ul>
      <li>and pseudo count?</li>
    </ul>
  </li>
  <li>max entropy</li>
</ul>

<hr />

<p>I expect that intrinsic motivation exploration strategies will be highly dependent on their past.
If it sees a few rewards for doing X, then it will continue to explore within X, possibly getting more rewards.
Positive feedback.</p>

<p>Is also dependent on how the value fn approximator generalises the rewards it has seen.</p>

<hr />

\[\begin{aligned}
P^{\pi}(\tau | \pi) = d_0(s_0) \Pi_{t=0}^{\infty} \pi(a_t | s_t)P(s_{t+1} | s_t, a_t) \\
d^{\pi}(s, t) = \sum_{\text{all $\tau$ with $s = s_t$}}P^{\pi}(\tau | \pi) \\
d^{\pi}(s) = (1-\gamma)\sum_{t=0}^{\infty} \gamma^t d^{\pi}(s, t) \\
\end{aligned}\]

<p>Does this discounted state distribution really make sense???</p>

<p>Convergence
\(KL(d^{\pi}(s, t), d^{\pi}(s))\)</p>

<hr />

<ul>
  <li>note. but we dont just care about exploring states??? inductive bias in state-actions?</li>
</ul>

<hr />

<p>Relationship to the heat equation?!
\(\frac{\partial d}{\partial t} = \alpha \frac{\partial^2 d}{\partial s^2}\)</p>

<hr />

<p>Want: for a given finite time horizon, the state visitation distribution is approimately max entropy. If we only require convergence in the limit, we could</p>

<p>Also, algorithms with short memories may forget.</p>

<hr />

<h2 id="what-do-we-require-from-an-exploration-strategy">What do we require from an exploration strategy?</h2>

<ul>
  <li>Non-zero probability of reaching all states, and trying all actions in each state.</li>
</ul>

<p>Nice to have</p>

<ul>
  <li>Converges to a uniform distribution over states.</li>
  <li>Scales sub-linearly with states</li>
  <li>Samples according to uncertainty.</li>
</ul>
:ET