I"“<p><a href="https://arxiv.org/abs/1507.02672">Ladder nets</a> (aka <a href="https://arxiv.org/abs/1505.04597">Unets</a>) are currently achieving state-of-the art results on image to image translation, for example <a href="https://phillipi.github.io/pix2pix/">see here</a>. This network architecture achieves such great results because the skip connetions allow higher frequency information, such as edges and gradients, to be easily communicated between the encoder and decoder. This is useful for image to image translation as typically we are translating between â€˜stylesâ€™ (low frequency content such as palette, textures), while the high frequency content maps to the â€˜contentâ€™ (see <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf">style transfer</a>). $\textbf{Q:}$ How can we use this architecutre for unsupervised learning?</p>
:ET