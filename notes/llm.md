The Question: Can We Design a "Market for Truth" to Fight Disinformation?

    Why it fits: This combines your interest in market design ("The cure for capitalism") with the philosophy of truth ("What makes something true?"). Current social media markets optimize for engagement, not truth, leading to the proliferation of misinformation. What would a market that properly incentivizes the creation and propagation of verified, nuanced information look like? Could we use prediction markets, bounties for falsification (a "market for Karl Popper"), or create a new type of "truth derivative" that pays out based on the long-term accuracy of claims? This tackles a core societal problem using your preferred toolkit: mechanism design.

    The Question: If a scientific proof is a public good, how can we create a market to properly incentivize its production without distorting it?

        Why it fits: This combines your interest in Mechanism Design (The cure for capitalism) with your deep dives into the epistemology of science (What makes something true?, Proving AGW). Currently, science is funded through grants and prestige—inefficient systems that can reward hype over rigor. What if we designed a market where:

            Well-posed conjectures (like the Riemann Hypothesis) have massive, cryptographically secured bounties.

            "Falsification bonds" are created, where individuals can bet against a prevailing theory, with the payout coming if it's overturned.

            A "Replication Market" pays out for successfully replicating or failing to replicate key experiments, directly rewarding the unglamorous but vital work of verification.

        This reframes scientific progress as an economic engineering problem. It forces you to confront the dark side: Could such a market be gamed? What happens when a hedge fund tries to short-sell the Standard Model of physics? It's a perfect blend of your formal, economic, and philosophical interests.


The Question: What is the "Cognitive Load" of a Law? Can a Society Suffer from Burnout?

    Why it fits: This bridges your work on Approximate Reasoning with your interest in societal design and regulation. You argue humans use heuristics because of cognitive limits. What are the cognitive limits of a society? We design laws, regulations, and tax codes of immense complexity. At what point does the collective cognitive burden of understanding and complying with these systems become so high that society as a whole resorts to "heuristics"—widespread non-compliance, loopholes, and black markets? Could we measure the "Kolmogorov complexity" of a legal code as a proxy for its cognitive load? This reframes legal design not as a moral or political problem, but as a cognitive engineering problem.


1. The Market for Proofs: Can We Design an Economy for Scientific Truth?

You've explored how markets can solve failures in capitalism and how science is limited by its ability to prove things. Let's merge these ideas.

    The Question: If a scientific proof is a public good, how can we create a market to properly incentivize its production without distorting it?

        Why it fits: This combines your interest in Mechanism Design (The cure for capitalism) with your deep dives into the epistemology of science (What makes something true?, Proving AGW). Currently, science is funded through grants and prestige—inefficient systems that can reward hype over rigor. What if we designed a market where:

            Well-posed conjectures (like the Riemann Hypothesis) have massive, cryptographically secured bounties.

            "Falsification bonds" are created, where individuals can bet against a prevailing theory, with the payout coming if it's overturned.

            A "Replication Market" pays out for successfully replicating or failing to replicate key experiments, directly rewarding the unglamorous but vital work of verification.

        This reframes scientific progress as an economic engineering problem. It forces you to confront the dark side: Could such a market be gamed? What happens when a hedge fund tries to short-sell the Standard Model of physics? It's a perfect blend of your formal, economic, and philosophical interests.


2. The Mathematics of Cognitive Bias: What is the Formal Object of a Heuristic?

You masterfully reframe cognitive biases as computationally rational strategies in "Approximate Reasoning." Let's take this one step further, in the style of your "Causal Calculus" or "Determinant and Laplacian" posts.

    The Question: What is the precise mathematical or algorithmic structure of a cognitive bias?

        Why it fits: You love finding deep, unifying formalisms. Instead of just saying a bias is "like" a shortcut, this question demands you define the shortcut itself as a mathematical object. For example:

            Is the Availability Heuristic simply "sampling from a non-stationary distribution with a recency-weighted kernel"?

            Is Confirmation Bias formally equivalent to "variational inference with an improperly strong prior that is resistant to updates"?

            Can Attribute Substitution be modeled as "solving a relaxed version of a constrained optimization problem"?

        This project would involve taking the psychological phenomena you've described and attempting to find their formal duals in statistics, computer science, and information theory. It's a quest to build a "calculus of heuristics," which is perfectly aligned with your unique skill set.




4. The Optimizer's Curse for Altruists: Acting Under Value Uncertainty

You've critiqued utilitarianism through the lens of uncertainty about outcomes. Let's apply that same rigor to uncertainty about values.

    The Question: How should an agent act when it is not only uncertain about the world, but also uncertain about what "good" truly is?

        Why it fits: This is the deepest, most philosophical question here, and it gets to the heart of the alignment problem you've explored in The Alignment Problem and EA to AI. Standard RL assumes a fixed, known reward function. But in reality, our values are complex, evolving, and we can't write them down perfectly. This is the problem of value uncertainty.

            An agent that is 100% confident in a slightly wrong goal (e.g., maximizing paperclips) is maximally dangerous. An agent that is uncertain about its goal might be safer. It might hedge its bets, seek clarification, or act more cautiously.

            This leads to fascinating technical questions: How do you represent a probability distribution over reward functions? How do you plan when your utility function is a random variable? Does this formalize the notion of "humility" in an AI?

        This question transforms the alignment problem from "how do we specify the right goal?" to "how do we build agents that can act safely and effectively without ever knowing the perfect goal?" It's a profound shift in perspective that feels like a natural next step for your work.